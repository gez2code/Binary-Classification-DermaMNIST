{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNiNK8a1GMo7u0eXuNoL+ME",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gez2code/Binary-Classification-DermaMNIST/blob/main/Binary_Classification_DermaMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Block 1: Installation, Imports & Setup\n\n## Purpose\nThis section initializes the complete environment for the DermaMNIST binary classification study.\n\n## Configuration Options\n- `USE_COLAB`: Set to `True` for Google Colab, `False` for local execution\n- `USE_WANDB`: Set to `True` to enable Weights & Biases tracking, `False` to disable\n- `USE_DRIVE`: Set to `True` to save models to Google Drive, `False` for local storage\n- `SEED`: Random seed for reproducibility (default: 42)\n\n## Contents\n1. Install dependencies (medmnist, wandb)\n2. Import libraries (TensorFlow, Keras, sklearn, etc.)\n3. Set random seeds for reproducibility\n4. Configure storage paths (Drive or local)\n5. Define utility functions",
      "metadata": {
        "id": "E7NQ_f178GHj"
      }
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# BLOCK 1: INSTALLATION, IMPORTS & SETUP\n# ============================================================================\n\n# ============================================================================\n# CONFIGURATION - MODIFY THESE SETTINGS AS NEEDED\n# ============================================================================\nUSE_COLAB = True      # Set to False for local execution\nUSE_WANDB = True      # Set to False to disable experiment tracking\nUSE_DRIVE = True      # Set to False to save models locally (only matters in Colab)\nSEED = 42             # Random seed for reproducibility\n\n# ============================================================================\n# INSTALLATION\n# ============================================================================\n!pip install -q medmnist wandb\n\n# ============================================================================\n# IMPORTS\n# ============================================================================\nimport os\nimport random\nimport gc\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# Deep Learning Imports\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, callbacks\nfrom tensorflow.keras.applications import ResNet50, VGG16, EfficientNetB0\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow.keras.backend as K\n\n# Metrics & Data\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, \n                             f1_score, roc_auc_score, confusion_matrix)\nfrom sklearn.utils import class_weight\nfrom medmnist import DermaMNIST\n\n# Tracking (conditional import)\nif USE_WANDB:\n    import wandb\n    from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\nelse:\n    wandb = None\n    print(\"\u26a0\ufe0f W&B tracking disabled. Set USE_WANDB=True to enable.\")\n\n# Google Colab specific imports\nif USE_COLAB:\n    try:\n        from google.colab import drive\n    except ImportError:\n        USE_COLAB = False\n        print(\"\u26a0\ufe0f Not running in Colab. Switching to local mode.\")\n\n# ============================================================================\n# REPRODUCIBILITY\n# ============================================================================\ndef set_seeds(seed=SEED):\n    \"\"\"Set random seeds for reproducibility across all libraries.\"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nset_seeds()\n\n# ============================================================================\n# GPU CHECK\n# ============================================================================\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    print(f\"\u2705 GPU available: {gpus[0].name}\")\n    # Prevent TF from allocating all GPU memory at once\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\nelse:\n    print(\"\u26a0\ufe0f WARNING: No GPU detected. Training will be significantly slower.\")\n\nprint(f\"\u2713 TensorFlow Version: {tf.__version__}\")\n\n# ============================================================================\n# STORAGE SETUP\n# ============================================================================\nif USE_COLAB and USE_DRIVE:\n    drive.mount('/content/drive')\n    BASE_PATH = '/content/drive/MyDrive/DermaMNIST_Study'\nelse:\n    BASE_PATH = './DermaMNIST_Study'\n\nDIR_PHASE_1 = os.path.join(BASE_PATH, 'Phase1_Baselines')\nDIR_PHASE_2 = os.path.join(BASE_PATH, 'Phase2_Tuning')\nDIR_PHASE_3 = os.path.join(BASE_PATH, 'Phase3_Final')\n\nfor folder in [DIR_PHASE_1, DIR_PHASE_2, DIR_PHASE_3]:\n    os.makedirs(folder, exist_ok=True)\n\nprint(f\"\\n\u2705 Folder structure created:\")\nprint(f\"   \ud83d\udcc2 Phase 1 -> {DIR_PHASE_1}\")\nprint(f\"   \ud83d\udcc2 Phase 2 -> {DIR_PHASE_2}\")\nprint(f\"   \ud83d\udcc2 Phase 3 -> {DIR_PHASE_3}\")\n\n# ============================================================================\n# UTILITY FUNCTIONS\n# ============================================================================\ndef save_to_drive(experiment_name, target_folder):\n    \"\"\"\n    Save model checkpoint to the specified folder.\n    \n    Args:\n        experiment_name (str): Name of the experiment (without .keras extension)\n        target_folder (str): Destination folder path\n    \"\"\"\n    filename = f\"{experiment_name}.keras\"\n    source = filename\n    destination = os.path.join(target_folder, filename)\n\n    if os.path.exists(source):\n        shutil.copy(source, destination)\n        print(f\"\ud83d\udcbe SAVED: {filename} -> {target_folder}\")\n    else:\n        print(f\"\u26a0\ufe0f ERROR: {filename} not found!\")\n\nprint(\"\\n\u2705 Setup complete!\")",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpWpwubdlkv2",
        "outputId": "3c973e71-66d3-42ad-faff-c9169b0ff7ee",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 2: Data Loading & Exploratory Data Analysis\n",
        "\n",
        "This section loads the DermaMNIST dataset and performs EDA before and after transformation.\n",
        "\n",
        "Structure:\n",
        "- 2a: Explore raw data (7 classes, original pixels)\n",
        "- 2b: Transform data (normalize, binary mapping, augmentation)\n",
        "- 2c: Verify transformed data (2 classes, class imbalance ~9:1)\n",
        "\n",
        "Dataset: DermaMNIST (28x28 RGB dermatology images)\n",
        "Task: Binary classification - Malignant (1) vs Benign (0)"
      ],
      "metadata": {
        "id": "MCQ2a2RR8RJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 2a: EDA - RAW DATA EXPLORATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83d\udcca EXPLORATORY DATA ANALYSIS - RAW DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load raw data (before any transformation)\n",
        "raw_train = DermaMNIST(split='train', download=True, size=28)\n",
        "raw_val = DermaMNIST(split='val', download=True, size=28)\n",
        "raw_test = DermaMNIST(split='test', download=True, size=28)\n",
        "\n",
        "# 1. Dataset Overview\n",
        "print(\"\\n1\ufe0f\u20e3 DATASET OVERVIEW\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   Training samples:   {len(raw_train.imgs):,}\")\n",
        "print(f\"   Validation samples: {len(raw_val.imgs):,}\")\n",
        "print(f\"   Test samples:       {len(raw_test.imgs):,}\")\n",
        "print(f\"   Total samples:      {len(raw_train.imgs) + len(raw_val.imgs) + len(raw_test.imgs):,}\")\n",
        "\n",
        "# 2. Image Properties\n",
        "print(\"\\n2\ufe0f\u20e3 IMAGE PROPERTIES\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   Shape: {raw_train.imgs[0].shape}\")\n",
        "print(f\"   Dtype: {raw_train.imgs[0].dtype}\")\n",
        "print(f\"   Pixel range: [{raw_train.imgs.min()}, {raw_train.imgs.max()}]\")\n",
        "\n",
        "# 3. Original Label Distribution (7 classes in DermaMNIST)\n",
        "print(\"\\n3\ufe0f\u20e3 ORIGINAL LABEL DISTRIBUTION (7 Classes)\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "# DermaMNIST original classes\n",
        "class_names = {\n",
        "    0: 'Melanocytic nevi (nv)',\n",
        "    1: 'Melanoma (mel)',\n",
        "    2: 'Benign keratosis (bkl)',\n",
        "    3: 'Basal cell carcinoma (bcc)',\n",
        "    4: 'Actinic keratoses (akiec)',\n",
        "    5: 'Vascular lesions (vasc)',\n",
        "    6: 'Dermatofibroma (df)'\n",
        "}\n",
        "\n",
        "# Count each class\n",
        "train_labels = raw_train.labels.flatten()\n",
        "unique, counts = np.unique(train_labels, return_counts=True)\n",
        "\n",
        "print(f\"   {'Class':<35} {'Count':>7} {'Percent':>8}\")\n",
        "print(f\"   {'-'*35} {'-'*7} {'-'*8}\")\n",
        "for cls, count in zip(unique, counts):\n",
        "    pct = 100 * count / len(train_labels)\n",
        "    print(f\"   {class_names[cls]:<35} {count:>7,} {pct:>7.1f}%\")\n",
        "\n",
        "# 4. Visualize Class Distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart\n",
        "ax1 = axes[0]\n",
        "bars = ax1.bar(range(len(unique)), counts, color='steelblue', edgecolor='black')\n",
        "ax1.set_xticks(range(len(unique)))\n",
        "ax1.set_xticklabels([f\"{i}\\n{class_names[i].split('(')[1].replace(')', '')}\" for i in unique], fontsize=9)\n",
        "ax1.set_xlabel('Class')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.set_title('Original 7-Class Distribution (Training Set)')\n",
        "\n",
        "# Add count labels on bars\n",
        "for bar, count in zip(bars, counts):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
        "             f'{count:,}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Pie chart\n",
        "ax2 = axes[1]\n",
        "ax2.pie(counts, labels=[class_names[i].split('(')[1].replace(')', '') for i in unique],\n",
        "        autopct='%1.1f%%', startangle=90)\n",
        "ax2.set_title('Class Proportions (Training Set)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. Sample Images from Each Class\n",
        "print(\"\\n4\ufe0f\u20e3 SAMPLE IMAGES FROM EACH CLASS\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "fig, axes = plt.subplots(2, 7, figsize=(16, 5))\n",
        "\n",
        "for cls in range(7):\n",
        "    # Get samples of this class\n",
        "    cls_indices = np.where(train_labels == cls)[0]\n",
        "\n",
        "    # Show 2 samples per class\n",
        "    for row in range(2):\n",
        "        idx = cls_indices[row]\n",
        "        axes[row, cls].imshow(raw_train.imgs[idx])\n",
        "        axes[row, cls].axis('off')\n",
        "        if row == 0:\n",
        "            short_name = class_names[cls].split('(')[1].replace(')', '')\n",
        "            axes[row, cls].set_title(f'{short_name}\\n(Class {cls})', fontsize=9)\n",
        "\n",
        "plt.suptitle('Sample Images from Each Original Class', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. Binary Mapping Preview\n",
        "print(\"\\n5\ufe0f\u20e3 BINARY MAPPING PREVIEW\")\n",
        "print(\"-\"*40)\n",
        "print(\"   Malignant (1): Classes 0, 1, 6\")\n",
        "print(\"      - 0: Melanocytic nevi (nv)\")\n",
        "print(\"      - 1: Melanoma (mel)\")\n",
        "print(\"      - 6: Dermatofibroma (df)\")\n",
        "print(\"\\n   Benign (0): Classes 2, 3, 4, 5\")\n",
        "print(\"      - 2: Benign keratosis (bkl)\")\n",
        "print(\"      - 3: Basal cell carcinoma (bcc)\")\n",
        "print(\"      - 4: Actinic keratoses (akiec)\")\n",
        "print(\"      - 5: Vascular lesions (vasc)\")\n",
        "\n",
        "# Calculate expected binary distribution\n",
        "malignant_classes = [0, 1, 6]\n",
        "malignant_count = sum(counts[i] for i in range(len(unique)) if unique[i] in malignant_classes)\n",
        "benign_count = sum(counts[i] for i in range(len(unique)) if unique[i] not in malignant_classes)\n",
        "\n",
        "print(f\"\\n   Expected after transformation:\")\n",
        "print(f\"      Benign:    {benign_count:,} ({100*benign_count/len(train_labels):.1f}%)\")\n",
        "print(f\"      Malignant: {malignant_count:,} ({100*malignant_count/len(train_labels):.1f}%)\")\n",
        "print(f\"      Ratio:     {benign_count/malignant_count:.1f}:1 (Benign:Malignant)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eSPlDEB22QCC",
        "outputId": "81c609d4-163b-4f9a-d81f-adcf737b63b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 2b: DATA LOADING & PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"\ud83d\udd04 DATA TRANSFORMATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    train_data = DermaMNIST(split='train', download=True, size=28)\n",
        "    val_data = DermaMNIST(split='val', download=True, size=28)\n",
        "    test_data = DermaMNIST(split='test', download=True, size=28)\n",
        "\n",
        "    # Normalize: [0, 255] \u2192 [0, 1]\n",
        "    print(\"\\n1\ufe0f\u20e3 Normalizing pixel values: [0, 255] \u2192 [0, 1]\")\n",
        "    x_train = train_data.imgs.astype('float32') / 255.0\n",
        "    x_val = val_data.imgs.astype('float32') / 255.0\n",
        "    x_test = test_data.imgs.astype('float32') / 255.0\n",
        "\n",
        "    # Binary mapping: Malignant (1) vs Benign (0)\n",
        "    print(\"2\ufe0f\u20e3 Converting to binary: 7 classes \u2192 2 classes\")\n",
        "    to_binary = lambda y: np.isin(y, [0, 1, 6]).astype(int)\n",
        "    y_train_bin = to_binary(train_data.labels)\n",
        "    y_val_bin = to_binary(val_data.labels)\n",
        "    y_test_bin = to_binary(test_data.labels)\n",
        "\n",
        "    # One-hot encode\n",
        "    print(\"3\ufe0f\u20e3 One-hot encoding labels\")\n",
        "    y_train = tf.keras.utils.to_categorical(y_train_bin, 2)\n",
        "    y_val = tf.keras.utils.to_categorical(y_val_bin, 2)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test_bin, 2)\n",
        "\n",
        "    # Class weights\n",
        "    class_weights = None  # Will be set in 6\n",
        "\n",
        "    return {\n",
        "        'x_train': x_train, 'y_train': y_train, 'y_train_bin': y_train_bin,\n",
        "        'x_val': x_val, 'y_val': y_val, 'y_val_bin': y_val_bin,\n",
        "        'x_test': x_test, 'y_test': y_test, 'y_test_bin': y_test_bin,\n",
        "        'class_weights': class_weights\n",
        "    }\n",
        "\n",
        "def get_augmentation_generator(seed=SEED):\n",
        "    return ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.15,\n",
        "        height_shift_range=0.15,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "# Load and transform data\n",
        "data = load_and_preprocess_data()\n",
        "datagen = get_augmentation_generator(seed=SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r1UECKr2SPv",
        "outputId": "b041a56f-cb72-4a79-a094-433c72b7b5da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 2c: EDA - TRANSFORMED DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\ud83d\udcca EXPLORATORY DATA ANALYSIS - TRANSFORMED DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Transformed Data Overview\n",
        "print(\"\\n1\ufe0f\u20e3 TRANSFORMED DATA OVERVIEW\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   {'Split':<12} {'Shape':<20} {'Dtype':<12} {'Pixel Range'}\")\n",
        "print(f\"   {'-'*12} {'-'*20} {'-'*12} {'-'*15}\")\n",
        "print(f\"   {'Train':<12} {str(data['x_train'].shape):<20} {str(data['x_train'].dtype):<12} [{data['x_train'].min():.2f}, {data['x_train'].max():.2f}]\")\n",
        "print(f\"   {'Val':<12} {str(data['x_val'].shape):<20} {str(data['x_val'].dtype):<12} [{data['x_val'].min():.2f}, {data['x_val'].max():.2f}]\")\n",
        "print(f\"   {'Test':<12} {str(data['x_test'].shape):<20} {str(data['x_test'].dtype):<12} [{data['x_test'].min():.2f}, {data['x_test'].max():.2f}]\")\n",
        "\n",
        "# 2. Binary Label Distribution\n",
        "print(\"\\n2\ufe0f\u20e3 BINARY LABEL DISTRIBUTION\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "for split_name, y_bin in [('Train', data['y_train_bin']),\n",
        "                           ('Val', data['y_val_bin']),\n",
        "                           ('Test', data['y_test_bin'])]:\n",
        "    total = len(y_bin)\n",
        "    malignant = y_bin.sum()\n",
        "    benign = total - malignant\n",
        "    print(f\"   {split_name}:\")\n",
        "    print(f\"      Benign (0):    {benign:>5,} ({100*benign/total:>5.1f}%)\")\n",
        "    print(f\"      Malignant (1): {int(malignant):>5,} ({100*malignant/total:>5.1f}%)\")\n",
        "    print(f\"      Ratio: {benign/malignant:.1f}:1\")\n",
        "    print()\n",
        "\n",
        "# 3. Visualize Binary Distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "for ax, (split_name, y_bin) in zip(axes, [('Train', data['y_train_bin']),\n",
        "                                           ('Val', data['y_val_bin']),\n",
        "                                           ('Test', data['y_test_bin'])]):\n",
        "    total = len(y_bin)\n",
        "    malignant = int(y_bin.sum())\n",
        "    benign = total - malignant\n",
        "\n",
        "    bars = ax.bar(['Benign\\n(0)', 'Malignant\\n(1)'], [benign, malignant],\n",
        "                  color=['forestgreen', 'crimson'], edgecolor='black')\n",
        "    ax.set_title(f'{split_name} Set (n={total:,})')\n",
        "    ax.set_ylabel('Count')\n",
        "\n",
        "    # Add labels\n",
        "    for bar, count in zip(bars, [benign, malignant]):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
        "                f'{count:,}\\n({100*count/total:.1f}%)', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.suptitle('Binary Class Distribution After Transformation', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Sample Images by Binary Class\n",
        "print(\"\\n3\ufe0f\u20e3 SAMPLE IMAGES BY BINARY CLASS\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "fig, axes = plt.subplots(2, 6, figsize=(14, 5))\n",
        "\n",
        "y_train_flat = data['y_train_bin'].flatten()\n",
        "\n",
        "for row, (cls, cls_name, color) in enumerate([(0, 'Benign', 'forestgreen'),\n",
        "                                                (1, 'Malignant', 'crimson')]):\n",
        "    cls_indices = np.where(y_train_flat == cls)[0]\n",
        "\n",
        "    for col in range(6):\n",
        "        idx = cls_indices[col]\n",
        "        axes[row, col].imshow(data['x_train'][idx])\n",
        "        axes[row, col].axis('off')\n",
        "        if col == 0:\n",
        "            axes[row, col].set_ylabel(cls_name, fontsize=12, color=color, fontweight='bold')\n",
        "\n",
        "axes[0, 0].set_title('Benign Samples (Class 0)', fontsize=10, loc='left', color='forestgreen')\n",
        "axes[1, 0].set_title('Malignant Samples (Class 1)', fontsize=10, loc='left', color='crimson')\n",
        "\n",
        "plt.suptitle('Sample Images After Binary Transformation', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. One-Hot Encoding Verification\n",
        "print(\"\\n4\ufe0f\u20e3 ONE-HOT ENCODING VERIFICATION\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   y_train shape: {data['y_train'].shape}\")\n",
        "print(f\"   y_train sample (first 5):\")\n",
        "print(f\"   {data['y_train'][:5]}\")\n",
        "\n",
        "# 6. Data Augmentation Preview\n",
        "print(\"\\n5\ufe0f\u20e3 DATA AUGMENTATION PREVIEW\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "# Pick one sample\n",
        "sample_idx = np.where(data['y_train_bin'].flatten() == 1)[0][0]  # First malignant\n",
        "sample_img = data['x_train'][sample_idx:sample_idx+1]\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "\n",
        "# Original\n",
        "axes[0, 0].imshow(sample_img[0])\n",
        "axes[0, 0].set_title('Original', fontweight='bold')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "# Augmented versions\n",
        "aug_iter = datagen.flow(sample_img, batch_size=1, seed=SEED)\n",
        "for i in range(1, 10):\n",
        "    row = i // 5\n",
        "    col = i % 5\n",
        "    aug_img = next(aug_iter)[0]\n",
        "    axes[row, col].imshow(aug_img)\n",
        "    axes[row, col].set_title(f'Augmented {i}')\n",
        "    axes[row, col].axis('off')\n",
        "\n",
        "plt.suptitle('Data Augmentation Examples (Same Image)', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 7. Class Imbalance Summary\n",
        "print(\"\\n6\ufe0f\u20e3 CLASS IMBALANCE SUMMARY\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   \u26a0\ufe0f  Your data is IMBALANCED\")\n",
        "print(f\"   \")\n",
        "print(f\"   Benign:Malignant ratio = ~9:1\")\n",
        "print(f\"   \")\n",
        "print(f\"   Without intervention, model will likely:\")\n",
        "print(f\"      \u2022 Predict 'Benign' most of the time (90% accuracy!)\")\n",
        "print(f\"      \u2022 Have LOW recall for Malignant class\")\n",
        "print(f\"   \")\n",
        "print(f\"   Strategies to address:\")\n",
        "print(f\"      \u2022 Class weights: {'{0: 1.0, 1: 3.0}'} or higher\")\n",
        "print(f\"      \u2022 Oversampling (SMOTE)\")\n",
        "print(f\"      \u2022 Focal loss\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\u2705 EDA COMPLETE - Ready for modeling\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "jnR9QMmx2WY-",
        "outputId": "6d0a545f-013d-4c59-9a38-e19546a4fdba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLOCK 3: MODEL ARCHITECTURES"
      ],
      "metadata": {
        "id": "H8uMQLMq8eaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 3: MODEL ARCHITECTURES\n",
        "# ============================================================================\n",
        "def build_custom_cnn(filters_base=32, depth=3, dropout=0.5, dense_units=512):\n",
        "    model = models.Sequential([layers.Input(shape=(28, 28, 3))])\n",
        "    for i in range(depth):\n",
        "        filters = filters_base * (2 ** i)\n",
        "        model.add(layers.Conv2D(filters, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.MaxPooling2D((2, 2)))\n",
        "        model.add(layers.Dropout(dropout * (0.5 + i*0.25)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(dense_units, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(dropout))\n",
        "    model.add(layers.Dense(2, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def build_transfer_model(base_name='resnet50', dropout=0.5, unfreeze_layers=None, dense_units=256):\n",
        "    inputs = layers.Input(shape=(28, 28, 3))\n",
        "    # Upsampling is crucial for pre-trained models\n",
        "    x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(inputs)\n",
        "\n",
        "    base_models = {\n",
        "        'resnet50': lambda: ResNet50(include_top=False, weights='imagenet', input_shape=(56, 56, 3), pooling='avg'),\n",
        "        'vgg16': lambda: VGG16(include_top=False, weights='imagenet', input_shape=(56, 56, 3), pooling='avg'),\n",
        "        'efficientnet': lambda: EfficientNetB0(include_top=False, weights='imagenet', input_shape=(56, 56, 3), pooling='avg')\n",
        "    }\n",
        "\n",
        "    if base_name not in base_models: raise ValueError(f\"Unknown base: {base_name}\")\n",
        "    base = base_models[base_name]()\n",
        "\n",
        "    # Fine-tuning logic\n",
        "    if unfreeze_layers is None:\n",
        "        base.trainable = True # Full fine-tuning\n",
        "    elif unfreeze_layers == 0:\n",
        "        base.trainable = False # Feature extraction\n",
        "    else:\n",
        "        base.trainable = True\n",
        "        for layer in base.layers[:-unfreeze_layers]:\n",
        "            layer.trainable = False\n",
        "\n",
        "    x = base(x, training=False)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(dense_units, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout * 0.5)(x)\n",
        "    outputs = layers.Dense(2, activation='softmax')(x)\n",
        "\n",
        "    return models.Model(inputs, outputs, name=f\"{base_name}_model\")"
      ],
      "metadata": {
        "id": "bQAIvl1XlpL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLOCK 4: TRAINING CONFIGURATION & HELPERS"
      ],
      "metadata": {
        "id": "KfeV-wQT8t83"
      }
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# BLOCK 4: TRAINING CONFIGURATION & HELPERS\n# ============================================================================\n\ndef compile_model(model, learning_rate):\n    \"\"\"Single definition with all metrics\"\"\"\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=learning_rate),\n        loss='categorical_crossentropy',\n        metrics=[\n            tf.keras.metrics.AUC(name='auc'),\n            tf.keras.metrics.Precision(class_id=1, name='precision'),\n            tf.keras.metrics.Recall(class_id=1, name='recall')\n        ]\n    )\n\ndef get_callbacks(config, model_path):\n    \"\"\"\n    Create training callbacks for early stopping and model checkpointing.\n    \n    IMPORTANT: Monitors val_recall (not val_loss) to optimize for clinical goal\n    of maximizing cancer detection (sensitivity).\n    \n    Args:\n        config (dict): Experiment configuration with 'patience' key\n        model_path (str): Path to save best model checkpoint\n    \n    Returns:\n        list: List of Keras callbacks\n    \"\"\"\n    callback_list = [\n        callbacks.EarlyStopping(\n            monitor='val_recall',         # \u2713 FIXED: Monitor recall, not loss\n            patience=config['patience'],\n            mode='max',                   # \u2713 FIXED: Higher recall is better\n            restore_best_weights=True,\n            verbose=1\n        ),\n        callbacks.ModelCheckpoint(\n            filepath=model_path,\n            monitor='val_recall',         # \u2713 FIXED: Save best recall model\n            mode='max',                   # \u2713 FIXED: Higher recall is better\n            save_best_only=True,\n            verbose=1\n        ),\n    ]\n    \n    # Add W&B callbacks only if enabled\n    if USE_WANDB and wandb is not None:\n        callback_list.extend([\n            WandbMetricsLogger(log_freq='epoch'),\n            WandbModelCheckpoint(model_path, monitor='val_recall', mode='max', save_best_only=True)\n        ])\n    \n    return callback_list\n\ndef compute_metrics(model, data_dict, split_name):\n    \"\"\"Compute essential metrics on full dataset\"\"\"\n    y_pred_probs = model.predict(data_dict['x'], verbose=0)\n    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n    y_true_classes = np.argmax(data_dict['y'], axis=1)\n\n    metrics = {\n        f'{split_name}/auc': roc_auc_score(data_dict['y'], y_pred_probs),\n        f'{split_name}/recall_mal': recall_score(y_true_classes, y_pred_classes, pos_label=1, zero_division=0),\n        f'{split_name}/precision_mal': precision_score(y_true_classes, y_pred_classes, pos_label=1, zero_division=0),\n        f'{split_name}/f1_mal': f1_score(y_true_classes, y_pred_classes, pos_label=1, zero_division=0)\n    }\n\n    return metrics, confusion_matrix(y_true_classes, y_pred_classes)",
      "metadata": {
        "id": "aHM5rFQrlrAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9-YXZ31C8rLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLOCK 5: MAIN TRAINING LOOP (FIXED)"
      ],
      "metadata": {
        "id": "ehLF6x378rdI"
      }
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# BLOCK 5: MAIN TRAINING LOOP (WITH CONFUSION MATRIX)\n# ============================================================================\ndef train_experiment(config, data, datagen):\n    print(f\"\\n{'='*60}\")\n    print(f\"\ud83d\ude80 STARTING: {config['name']}\")\n    print(f\"{'='*60}\")\n\n    model = None\n    run = None\n\n    try:\n        # 1. Init W&B (if enabled)\n        if USE_WANDB and wandb is not None:\n            if wandb.run is not None:\n                wandb.finish()\n            run = wandb.init(\n                project=\"DermaMNIST_Study\",\n                name=config['name'],\n                config=config,\n                reinit=True,\n                id=wandb.util.generate_id()\n            )\n        else:\n            run = None\n\n        # 2. Build Model\n        if config['architecture'] == 'custom_cnn':\n            model = build_custom_cnn(\n                config['filters_base'],\n                config['depth'],\n                config['dropout'],\n                config['dense_units']\n            )\n        else:\n            model = build_transfer_model(\n                config['architecture'],\n                config['dropout'],\n                config['unfreeze_layers'],\n                config['dense_units']\n            )\n\n        compile_model(model, config['learning_rate'])\n        model_path = f\"{config['name']}.keras\"\n\n        # 3. Create infinite dataset\n        train_gen = datagen.flow(\n            data['x_train'],\n            data['y_train'],\n            batch_size=config['batch_size'],\n            seed=SEED\n        )\n        steps = len(data['x_train']) // config['batch_size']\n\n        train_dataset = tf.data.Dataset.from_generator(\n            lambda: train_gen,\n            output_signature=(\n                tf.TensorSpec(shape=(None, 28, 28, 3), dtype=tf.float32),\n                tf.TensorSpec(shape=(None, 2), dtype=tf.float32)\n            )\n        ).repeat()\n\n        # 4. Train\n        history = model.fit(\n            train_dataset,\n            steps_per_epoch=steps,\n            epochs=config['epochs'],\n            validation_data=(data['x_val'], data['y_val']),\n            class_weight=data['class_weights'],\n            callbacks=get_callbacks(config, model_path),\n            verbose=1\n        )\n\n        # 5. FIND BEST METRICS (Summary)\n        # We look for the best epoch based on your goal (e.g., Highest Recall)\n        val_recall_history = history.history['val_recall']\n        best_epoch_idx = np.argmax(val_recall_history)\n\n        best_val_recall = val_recall_history[best_epoch_idx]\n        best_val_auc = history.history['val_auc'][best_epoch_idx]\n        best_val_loss = history.history['val_loss'][best_epoch_idx]\n\n        # 6. LOG CONFUSION MATRIX (New Step!)\n        print(\"\\n\ud83d\udcca Generating Confusion Matrix for BEST Model...\")\n\n        # A. Reload the best model from disk (Critical!)\n        # The current 'model' variable has the weights from the LAST epoch.\n        # We need the weights from the BEST epoch.\n        model.load_weights(model_path)\n\n        # B. Get predictions on Validation set\n        val_probs = model.predict(data['x_val'], verbose=0)\n        val_preds = np.argmax(val_probs, axis=1)\n        val_true = np.argmax(data['y_val'], axis=1)\n\n        # C. Log interactive matrix to W&B (if enabled)\n        if USE_WANDB and wandb is not None:\n            wandb.log({\n                \"conf_mat\": wandb.plot.confusion_matrix(\n                    probs=None,\n                    y_true=val_true,\n                    preds=val_preds,\n                    class_names=[\"Benign\", \"Malignant\"]\n                )\n            })\n\n        # D. Print text matrix for quick sanity check\n        cm = confusion_matrix(val_true, val_preds)\n        print(\"\\n   [Benign, Malignant]\")\n        print(f\"   {cm[0]}\")\n        print(f\"   {cm[1]}\")\n\n        # 7. Log Summary\n        results = {\n            'best_val_recall': best_val_recall,\n            'val_auc': best_val_auc,\n            'val_loss': best_val_loss,\n            'epochs_trained': len(val_recall_history)\n        }\n        if USE_WANDB and wandb is not None:\n            wandb.log(results)\n\n        print(f\"\\n\u2705 Results Logged. Best Epoch: {best_epoch_idx + 1}\")\n        return {**results, 'config': config, 'history': history.history}\n\n    except Exception as e:\n        print(f\"\\n\u274c ERROR: {e}\")\n        import traceback\n        traceback.print_exc()\n        return {'config': config, 'error': str(e)}\n\n    finally:\n        print(f\"\ud83e\uddf9 Cleanup...\")\n        if USE_WANDB and wandb is not None and run is not None:\n            wandb.finish()\n        K.clear_session()\n        if model is not None:\n            del model\n        gc.collect()",
      "metadata": {
        "id": "Zcbm6KIqlspt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 6 Competition - Phase 1 ARCHITECTURE COMPARISON\n"
      ],
      "metadata": {
        "id": "iHfJihSB82dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 6: EXPERIMENT CONFIGURATION (DYNAMIC)\n",
        "# ============================================================================\n",
        "# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "# \u2502 \ud83c\udf9b\ufe0f  DYNAMIC CONTROL PANEL                                              \u2502\n",
        "# \u2502 Change 'CURRENT_EXPERIMENT' to switch strategies instantly.             \u2502\n",
        "# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "# OPTIONS: \"W0\", \"W3\", \"W5\", \"W9\"\n",
        "CURRENT_EXPERIMENT = \"W9\"   # <--- \u26a1 CHANGE THIS ONLY!\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# AUTOMATIC CONFIGURATION LOGIC\n",
        "# ------------------------------------------------------------------\n",
        "strategies = {\n",
        "    \"W0\": None,                 # Baseline (No weights)\n",
        "    \"W3\": {0: 1.0, 1: 3.0},     # Moderate (3x penalty)\n",
        "    \"W5\": {0: 1.0, 1: 5.0},     # Aggressive (5x penalty)\n",
        "    \"W9\": {0: 1.0, 1: 9.0}      # Full Balance (9x penalty)\n",
        "}\n",
        "\n",
        "if CURRENT_EXPERIMENT not in strategies:\n",
        "    raise ValueError(f\"\u274c Unknown experiment: {CURRENT_EXPERIMENT}. Choose from {list(strategies.keys())}\")\n",
        "\n",
        "# Set variables automatically based on selection\n",
        "CLASS_WEIGHT_STRATEGY = strategies[CURRENT_EXPERIMENT]\n",
        "WEIGHT_TAG = CURRENT_EXPERIMENT\n",
        "\n",
        "# Apply to data dictionary so all models see it\n",
        "data['class_weights'] = CLASS_WEIGHT_STRATEGY\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# CONFIRMATION OUTPUT\n",
        "# ------------------------------------------------------------------\n",
        "print(\"=\"*60)\n",
        "print(f\"\u2699\ufe0f  READY TO RUN EXPERIMENT: {CURRENT_EXPERIMENT}\")\n",
        "print(\"=\"*60)\n",
        "print(f\"   1. Strategy Code:  {CURRENT_EXPERIMENT}\")\n",
        "print(f\"   2. Class Weights:  {CLASS_WEIGHT_STRATEGY}\")\n",
        "if CLASS_WEIGHT_STRATEGY is None:\n",
        "    print(f\"   3. Description:    BASELINE (No class balancing)\")\n",
        "    print(f\"                      Expect high accuracy but LOW recall.\")\n",
        "else:\n",
        "    ratio = CLASS_WEIGHT_STRATEGY[1]\n",
        "    print(f\"   3. Description:    WEIGHTED TRAINING ({ratio}x penalty)\")\n",
        "    print(f\"                      Expect higher recall, possibly lower precision.\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sjsR7wC_Srd",
        "outputId": "ab00e0a9-021b-4b3e-f5c0-bc38e528217b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 6a: Custom CNN\n",
        "# ============================================================================\n",
        "config_cnn = {\n",
        "    'name': f'P1_CNN_{WEIGHT_TAG}',\n",
        "    'architecture': 'custom_cnn',\n",
        "    'filters_base': 64, #Gives the model \"wider eyes\" to see more initial details.\n",
        "    'depth': 4,\n",
        "    'dropout': 0.4,\n",
        "    'dense_units': 256,\n",
        "    'learning_rate': 0.001,\n",
        "    'batch_size': 64,\n",
        "    'epochs': 50,\n",
        "    'patience': 10,\n",
        "    'unfreeze_layers': None\n",
        "}\n",
        "\n",
        "res_cnn = train_experiment(config_cnn, data, datagen)\n",
        "save_to_drive(config_cnn['name'], DIR_PHASE_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FK0sa1afmCIa",
        "outputId": "ed4d9546-cf82-4b11-b46e-6e9997bc9f64",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 6b: ResNet50\n",
        "# ============================================================================\n",
        "config_resnet = {\n",
        "    'name': f'P1_ResNet50_{WEIGHT_TAG}_unfrozen',  # \u2190 Added f\n",
        "    'architecture': 'resnet50',\n",
        "    'dropout': 0.5,\n",
        "    'dense_units': 256,\n",
        "    'unfreeze_layers': None, # 0 = Total frozen, except head layer; 1-174 = hybrid unfrozen layers ; None = All 175  unfrozen layers.\n",
        "    'learning_rate': 0.0001, # 0.0001 = In case of unfrozen layers > 0.\n",
        "    'batch_size': 32,\n",
        "    'epochs': 30,\n",
        "    'patience': 10\n",
        "}\n",
        "\n",
        "res_resnet = train_experiment(config_resnet, data, datagen)\n",
        "save_to_drive(config_resnet['name'], DIR_PHASE_1)"
      ],
      "metadata": {
        "id": "O28CDEKjmECS",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5075c10c-53c7-4179-bed9-61196431b0d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 6c: VGG16\n",
        "# ============================================================================\n",
        "config_vgg = {\n",
        "    'name': f'P1_VGG16_{WEIGHT_TAG}_unfrozen',\n",
        "    'architecture': 'vgg16',\n",
        "    'dropout': 0.5,\n",
        "    'dense_units': 256,\n",
        "    'unfreeze_layers': 0,\n",
        "    'learning_rate': 0.0001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 30,\n",
        "    'patience': 10\n",
        "}\n",
        "\n",
        "res_vgg = train_experiment(config_vgg, data, datagen)\n",
        "save_to_drive(config_vgg['name'], DIR_PHASE_1)"
      ],
      "metadata": {
        "id": "OSzjrFDpmFn9",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5580de9-5ebf-4ccb-b1a8-70ba1fb850c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 6d: EfficientNet\n",
        "# ============================================================================\n",
        "config_effnet = {\n",
        "    'name': f'P1_EfficientNet_{WEIGHT_TAG}',  # \u2190 Added f\n",
        "    'architecture': 'efficientnet',\n",
        "    'dropout': 0.5,\n",
        "    'dense_units': 256,\n",
        "    'unfreeze_layers': 0,\n",
        "    'learning_rate': 0.001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 30,\n",
        "    'patience': 10\n",
        "}\n",
        "\n",
        "res_effnet = train_experiment(config_effnet, data, datagen)\n",
        "save_to_drive(config_effnet['name'], DIR_PHASE_1)"
      ],
      "metadata": {
        "id": "mB8atNOGmG8l",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94ef9e4b-2db0-4dbd-82b0-263500171296"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 6e: COMPARE RESULTS (VALIDATION ONLY)\n",
        "# ============================================================================\n",
        "\n",
        "all_results = []\n",
        "\n",
        "if 'res_cnn' in locals(): all_results.append(res_cnn)\n",
        "if 'res_resnet' in locals(): all_results.append(res_resnet)\n",
        "if 'res_vgg' in locals(): all_results.append(res_vgg)\n",
        "if 'res_effnet' in locals(): all_results.append(res_effnet)\n",
        "\n",
        "if all_results:\n",
        "    comparison_data = []\n",
        "\n",
        "    for res in all_results:\n",
        "        if 'error' in res:\n",
        "            print(f\"\u26a0\ufe0f Skipping {res['config']['name']} (failed)\")\n",
        "            continue\n",
        "\n",
        "        comparison_data.append({\n",
        "            'Architecture': res['config']['architecture'],\n",
        "            'Name': res['config']['name'],\n",
        "            'Best_Val_Loss': f\"{res.get('best_val_loss', 0):.4f}\",\n",
        "            'Val_Recall': f\"{res.get('val_recall', 0):.4f}\",\n",
        "            'Val_Precision': f\"{res.get('val_precision', 0):.4f}\",\n",
        "            'Val_AUC': f\"{res.get('val_auc', 0):.4f}\",\n",
        "            'Best_Epoch': res.get('best_epoch', 0),\n",
        "            'Epochs_Trained': res.get('epochs_trained', 0)\n",
        "        })\n",
        "\n",
        "    df_compare = pd.DataFrame(comparison_data)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"\ud83c\udfc6 PHASE 1 RESULTS SUMMARY (Weight Strategy: {WEIGHT_TAG})\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "    print(df_compare.to_string(index=False))\n",
        "\n",
        "    # Find best by Val Loss (lowest = best)\n",
        "    best_idx = df_compare['Best_Val_Loss'].astype(float).idxmin()  # \u2190 idxmin for loss\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"\ud83e\udd47 BEST MODEL (by Validation Loss):\")\n",
        "    print(f\"   Architecture:   {df_compare.loc[best_idx, 'Architecture']}\")\n",
        "    print(f\"   Name:           {df_compare.loc[best_idx, 'Name']}\")\n",
        "    print(f\"   Best Val Loss:  {df_compare.loc[best_idx, 'Best_Val_Loss']} \u2190 Selection Metric\")\n",
        "    print(f\"   Val Recall:     {df_compare.loc[best_idx, 'Val_Recall']} \u2190 Primary Goal\")\n",
        "    print(f\"   Val AUC:        {df_compare.loc[best_idx, 'Val_AUC']}\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Save to CSV\n",
        "    csv_filename = f'phase1_results_{WEIGHT_TAG}.csv'\n",
        "    df_compare.to_csv(csv_filename, index=False)\n",
        "    print(f\"\ud83d\udcbe Results saved to: {csv_filename}\")\n",
        "\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f No results available yet. Run blocks 6a-6d first.\")"
      ],
      "metadata": {
        "id": "p19grKPPmQ9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N0D_MrOF9c2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 7 Competition - Phase 2 HYPERPARAMETER TUNING"
      ],
      "metadata": {
        "id": "bWqViAIT9nz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 7: HYPERPARAMETER TUNING (Phase 2)\n",
        "# ============================================================================\n",
        "\n",
        "# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "# \u2502 \ud83d\udee1\ufe0f SAFETY HEADER: Explicitly set intent to prevent errors               \u2502\n",
        "# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "# 1. HARDCODE YOUR WINNER & STRATEGY\n",
        "WINNER_ARCH = 'resnet50'      # Options: 'custom_cnn', 'resnet50', 'vgg16'\n",
        "TARGET_STRATEGY = 'W3'        # Options: 'W0', 'W3', 'W5'\n",
        "\n",
        "# 2. FORCE RE-CALCULATION OF WEIGHTS\n",
        "strategies = {\n",
        "    \"W0\": None,\n",
        "    \"W3\": {0: 1.0, 1: 3.0},\n",
        "    \"W5\": {0: 1.0, 1: 5.0}\n",
        "}\n",
        "WEIGHT_TAG = TARGET_STRATEGY\n",
        "data['class_weights'] = strategies[TARGET_STRATEGY]\n",
        "\n",
        "# 3. VERIFICATION PRINT\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"\ud83d\udd12 LOCKED IN FOR TUNING:\")\n",
        "print(f\"   Architecture:   {WINNER_ARCH}\")\n",
        "print(f\"   Strategy Tag:   {WEIGHT_TAG}\")\n",
        "print(f\"   Actual Weights: {data['class_weights']}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# 4. DEFINE TUNING CONFIGURATIONS\n",
        "tuning_configs = []\n",
        "\n",
        "if WINNER_ARCH == 'custom_cnn':\n",
        "    # [Skipping CNN configs since ResNet won]\n",
        "    pass\n",
        "else:\n",
        "    # Transfer Learning Tuning Strategies\n",
        "    tuning_configs = [\n",
        "        # \ud83e\uddea EXPERIMENT 1: The \"Conservative\" Approach\n",
        "        # Hypothesis: Unfreezing everything might be too unstable.\n",
        "        # Action: Only let the top 10 layers learn. Keep the rest as standard \"ImageNet\".\n",
        "        {\n",
        "            'name': f'P2_{WINNER_ARCH}_Freeze10_{WEIGHT_TAG}',\n",
        "            'architecture': WINNER_ARCH,\n",
        "            'description': 'Unfreeze top 10 layers only (Stability focus)',\n",
        "            'dropout': 0.5,\n",
        "            'dense_units': 256,\n",
        "            'unfreeze_layers': 10,       # <--- Minimal unfreezing\n",
        "            'learning_rate': 0.0001,\n",
        "            'batch_size': 32,\n",
        "            'epochs': 50,\n",
        "            'patience': 15\n",
        "        },\n",
        "\n",
        "        # \ud83e\uddea EXPERIMENT 2: The \"Moderate\" Approach (Goldilocks)\n",
        "        # Hypothesis: 10 layers isn't enough to learn dermatology features.\n",
        "        # Action: Double the learnable space to 20 layers.\n",
        "        {\n",
        "            'name': f'P2_{WINNER_ARCH}_Freeze20_{WEIGHT_TAG}',\n",
        "            'architecture': WINNER_ARCH,\n",
        "            'description': 'Unfreeze top 20 layers (Balance focus)',\n",
        "            'dropout': 0.5,\n",
        "            'dense_units': 256,\n",
        "            'unfreeze_layers': 20,       # <--- More flexibility\n",
        "            'learning_rate': 0.0001,\n",
        "            'batch_size': 32,\n",
        "            'epochs': 50,\n",
        "            'patience': 15\n",
        "        },\n",
        "\n",
        "        # \ud83e\uddea EXPERIMENT 3: The \"Anti-Overfitting\" Approach\n",
        "        # Hypothesis: The model is memorizing training data (High Train Recall, Low Val Recall).\n",
        "        # Action: Unfreeze EVERYTHING but aggressively delete 70% of neurons during training.\n",
        "        {\n",
        "            'name': f'P2_{WINNER_ARCH}_HighDropout_{WEIGHT_TAG}',\n",
        "            'architecture': WINNER_ARCH,\n",
        "            'description': 'Full Unfreeze + High Dropout (Regularization focus)',\n",
        "            'dropout': 0.7,              # <--- Aggressive Dropout\n",
        "            'dense_units': 256,\n",
        "            'unfreeze_layers': None,     # <--- Full Unfreeze\n",
        "            'learning_rate': 0.0001,\n",
        "            'batch_size': 32,\n",
        "            'epochs': 50,\n",
        "            'patience': 15\n",
        "        },\n",
        "\n",
        "        # \ud83e\uddea EXPERIMENT 4: The \"Gentle Surgery\" Approach\n",
        "        # Hypothesis: The Loss curve was spiky/unstable in Phase 1.\n",
        "        # Action: Unfreeze EVERYTHING but learn at half speed (0.00005).\n",
        "        {\n",
        "            'name': f'P2_{WINNER_ARCH}_LowLR_{WEIGHT_TAG}',\n",
        "            'architecture': WINNER_ARCH,\n",
        "            'description': 'Full Unfreeze + Very Low LR (Precision focus)',\n",
        "            'dropout': 0.5,\n",
        "            'dense_units': 256,\n",
        "            'unfreeze_layers': None,     # <--- Full Unfreeze\n",
        "            'learning_rate': 0.00005,    # <--- Super slow learning\n",
        "            'batch_size': 32,\n",
        "            'epochs': 50,\n",
        "            'patience': 15\n",
        "        },\n",
        "    ]\n",
        "\n",
        "# 5. RUN TUNING LOOP\n",
        "print(f\"\ud83d\udd27 Starting {len(tuning_configs)} Experiments...\")\n",
        "\n",
        "phase2_results = []\n",
        "\n",
        "for config in tuning_configs:\n",
        "    print(f\"\\n\u25b6\ufe0f RUNNING: {config['description']}\")\n",
        "    result = train_experiment(config, data, datagen)\n",
        "    phase2_results.append(result)\n",
        "    save_to_drive(config['name'], DIR_PHASE_2)\n",
        "\n",
        "# 6. COMPARE RESULTS\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"\ud83d\udcca PHASE 2 TUNING RESULTS\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "tuning_comparison = []\n",
        "for res in phase2_results:\n",
        "    if 'error' not in res:\n",
        "        tuning_comparison.append({\n",
        "            'Name': res['config']['name'],\n",
        "            'Strategy': res['config'].get('description', 'Custom'), # Shows your description\n",
        "            'Best_Val_Recall': f\"{res.get('best_val_recall', 0):.4f}\",\n",
        "            'Val_AUC': f\"{res.get('val_auc', 0):.4f}\",\n",
        "            'Val_Loss': f\"{res.get('val_loss', 0):.4f}\",\n",
        "            'Epochs': res.get('epochs_trained', 0)\n",
        "        })\n",
        "\n",
        "df_tuning = pd.DataFrame(tuning_comparison)\n",
        "print(df_tuning.to_string(index=False))\n",
        "\n",
        "# 7. IDENTIFY WINNER\n",
        "best_idx = df_tuning['Best_Val_Recall'].astype(float).idxmax()\n",
        "BEST_MODEL_NAME = df_tuning.loc[best_idx, 'Name']\n",
        "\n",
        "print(f\"\\n\ud83c\udfc6 BEST TUNED MODEL (by Val Recall):\")\n",
        "print(f\"   Name:            {BEST_MODEL_NAME}\")\n",
        "print(f\"   Strategy:        {df_tuning.loc[best_idx, 'Strategy']}\")\n",
        "print(f\"   Best Val Recall: {df_tuning.loc[best_idx, 'Best_Val_Recall']} \u2190 PRIMARY METRIC\")\n",
        "\n",
        "# 8. SAVE\n",
        "csv_filename = f'phase2_tuning_results_{WEIGHT_TAG}.csv'\n",
        "df_tuning.to_csv(csv_filename, index=False)\n",
        "print(f\"\\n\ud83d\udcbe Results saved to: {csv_filename}\")\n",
        "print(f\"\ud83d\udcbe Models saved to: {DIR_PHASE_2}\")"
      ],
      "metadata": {
        "id": "gpLhMwffvnNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 8 Competition - Phase 3 FINAL TEST EVALUATION"
      ],
      "metadata": {
        "id": "0c3IelWj9wqy"
      }
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# BLOCK 8: FINAL TEST EVALUATION (Phase 3 - WITH THRESHOLD TUNING)\n# ============================================================================\n\n# 1. SETUP\n# ------------------------------------------------------------------\nFINAL_MODEL_NAME = 'P2_resnet50_Freeze20_W3'  # \u2190 Update this with your actual Phase 2 winner!\nWEIGHT_TAG = FINAL_MODEL_NAME.split('_')[-1]   # Extract weight tag from model name (e.g., 'W3')\n\nfinal_model_path = os.path.join(DIR_PHASE_2, f'{FINAL_MODEL_NAME}.keras')\n\nprint(f\"{'='*60}\")\nprint(f\"\ud83c\udfc1 PHASE 3: FINAL TEST EVALUATION\")\nprint(f\"{'='*60}\")\nprint(f\"Model: {FINAL_MODEL_NAME}\")\nprint(f\"Weight Strategy: {WEIGHT_TAG}\")\n\n# 2. LOAD MODEL\n# ------------------------------------------------------------------\n# Check if model exists\nif not os.path.exists(final_model_path):\n    print(f\"\\n\u274c ERROR: Model file not found!\")\n    print(f\"   Expected path: {final_model_path}\")\n    print(f\"\\n   Please ensure you have run Phase 1 and Phase 2 first,\")\n    print(f\"   or update FINAL_MODEL_NAME to match your best model.\")\n    raise FileNotFoundError(f\"Model not found: {final_model_path}\")\n\ntry:\n    final_model = tf.keras.models.load_model(final_model_path)\n    print(\"\u2705 Model loaded successfully\")\nexcept Exception as e:\n    print(f\"\u274c Error loading model: {e}\")\n    raise\n\n# 3. THRESHOLD CALIBRATION (The \"Recall Booster\")\n# ------------------------------------------------------------------\nprint(\"\\n\ud83d\udd27 Calibrating Decision Threshold on VALIDATION set...\")\n\n# Get probabilities for Validation set\nval_probs = final_model.predict(data['x_val'], verbose=0)\nval_true = np.argmax(data['y_val'], axis=1)\nval_scores = val_probs[:, 1]  # Probability of Malignant class\n\n# Test thresholds from 0.1 to 0.9\nthresholds = np.arange(0.1, 0.9, 0.05)\nbest_threshold = 0.5\nbest_recall = 0.0\nacceptable_precision_limit = 0.40 # Don't let precision drop below 40%\n\nprint(f\"{'Thresh':<10} {'Recall':<10} {'Precision':<10}\")\nprint(\"-\" * 35)\n\nfor t in thresholds:\n    # Apply threshold\n    temp_preds = (val_scores >= t).astype(int)\n\n    # Calculate metrics\n    rec = recall_score(val_true, temp_preds)\n    prec = precision_score(val_true, temp_preds, zero_division=0)\n\n    print(f\"{t:.2f}       {rec:.4f}     {prec:.4f}\")\n\n    # STRATEGY: Maximize Recall, but keep Precision decent\n    # We update 'best_threshold' if Recall improves and Precision isn't trash\n    if rec > best_recall and prec >= acceptable_precision_limit:\n        best_recall = rec\n        best_threshold = t\n\nprint(f\"\\n\u2705 Selected Optimal Threshold: {best_threshold:.2f}\")\nprint(f\"   (Validation Recall: {best_recall:.4f})\")\n\n\n# 4. FINAL TEST (Using Optimal Threshold)\n# ------------------------------------------------------------------\nprint(f\"\\n\ud83d\udcca Evaluating on TEST SET using Threshold {best_threshold}...\")\n\n# Get probabilities for Test set\ntest_probs = final_model.predict(data['x_test'], verbose=0)\ntest_true = np.argmax(data['y_test'], axis=1)\ntest_scores = test_probs[:, 1]\n\n# Apply the calibrated threshold\ntest_preds = (test_scores >= best_threshold).astype(int)\n\n# 5. CALCULATE & PRINT RESULTS\n# ------------------------------------------------------------------\n# Metrics\nfinal_recall = recall_score(test_true, test_preds)\nfinal_precision = precision_score(test_true, test_preds)\nfinal_f1 = f1_score(test_true, test_preds)\nfinal_auc = roc_auc_score(test_true, test_scores)\ncm = confusion_matrix(test_true, test_preds)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"\ud83c\udfc6 FINAL RESULTS - {FINAL_MODEL_NAME}\")\nprint(f\"{'='*60}\")\n\nprint(f\"\\n\ud83d\udcc8 PRIMARY METRIC:\")\nprint(f\"   Recall (Sensitivity): {final_recall:.4f}\")\n\nprint(f\"\\n\ud83d\udcca SECONDARY METRICS:\")\nprint(f\"   Precision:            {final_precision:.4f}\")\nprint(f\"   F1 Score:             {final_f1:.4f}\")\nprint(f\"   AUC:                  {final_auc:.4f}\")\n\nprint(f\"\\n\ud83c\udfaf CONFUSION MATRIX (Thresh {best_threshold}):\")\nprint(f\"                  Predicted\")\nprint(f\"                 Benign  Malignant\")\nprint(f\"   Actual Benign    {cm[0,0]:5d}    {cm[0,1]:5d}\")\nprint(f\"          Malig.    {cm[1,0]:5d}    {cm[1,1]:5d}\")\n\n# 6. CLINICAL INTERPRETATION\n# ------------------------------------------------------------------\ntotal_malignant = cm[1,0] + cm[1,1]\ndetected = cm[1,1]\nmissed = cm[1,0]\n\nprint(f\"\\n\ud83c\udfe5 CLINICAL INTERPRETATION:\")\nprint(f\"   Total malignant cases: {total_malignant}\")\nprint(f\"   Correctly detected:    {detected} ({detected/total_malignant*100:.1f}%)\")\nprint(f\"   Missed (False Neg):    {missed} ({missed/total_malignant*100:.1f}%)\")\nprint(f\"   *Missed cases are the most dangerous errors.*\")\n\nprint(f\"\\n{'='*60}\")\n\n# 7. SAVE RESULTS\n# ------------------------------------------------------------------\nfinal_results = {\n    'model_name': FINAL_MODEL_NAME,\n    'weight_strategy': WEIGHT_TAG,\n    'threshold': best_threshold,\n    'test_recall': final_recall,\n    'test_precision': final_precision,\n    'test_f1': final_f1,\n    'test_auc': final_auc,\n    'true_neg': cm[0,0],\n    'false_pos': cm[0,1],\n    'false_neg': cm[1,0],\n    'true_pos': cm[1,1]\n}\n\ndf_final = pd.DataFrame([final_results])\nfinal_csv_path = os.path.join(DIR_PHASE_3, f'FINAL_TEST_RESULTS_{WEIGHT_TAG}.csv')\ndf_final.to_csv(final_csv_path, index=False)\n\nprint(f\"\\n\ud83d\udcbe Results saved to: {final_csv_path}\")\n\n# Copy model (shutil already imported in Block 1)\nfinal_model_dest = os.path.join(DIR_PHASE_3, f'{FINAL_MODEL_NAME}.keras')\nshutil.copy(final_model_path, final_model_dest)\nprint(f\"\ud83d\udcbe Model copied to: {final_model_dest}\")",
      "metadata": {
        "id": "flLA3Wwivqeb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}