{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è Configuration Guide\n",
        "\n",
        "**Before running this notebook, review and adjust the settings below based on your environment.**\n",
        "\n",
        "## Quick Setup\n",
        "\n",
        "| Setting | Google Colab | Local Machine |\n",
        "|---------|--------------|---------------|\n",
        "| `USE_COLAB` | `True` | `False` |\n",
        "| `USE_WANDB` | `True` (optional) | `True` (optional) |\n",
        "| `USE_DRIVE` | `True` (optional) | `False` |\n",
        "\n",
        "## Environment Options\n",
        "\n",
        "### 1. Execution Environment\n",
        "- **Google Colab (Recommended)**: Click the \"Open in Colab\" badge, set `USE_COLAB = True`\n",
        "- **Local Machine**: Clone repo, install requirements, set `USE_COLAB = False`\n",
        "\n",
        "### 2. Experiment Tracking (Weights & Biases)\n",
        "- **Enable**: Set `USE_WANDB = True`, you'll be prompted to login\n",
        "- **Disable**: Set `USE_WANDB = False` (no account needed)\n",
        "\n",
        "### 3. Model Storage\n",
        "- **Google Drive** (Colab only): Set `USE_DRIVE = True`\n",
        "- **Local Folder**: Set `USE_DRIVE = False` (saves to `./DermaMNIST_Study/`)\n",
        "\n",
        "### 4. GPU\n",
        "- **Colab**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
        "- **Local**: Requires NVIDIA GPU with CUDA\n",
        "\n",
        "---\n",
        "**Run the configuration cell below, then proceed with the notebook.**"
      ],
      "metadata": {
        "id": "config_guide"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Note: Configuration variables (USE_COLAB, USE_WANDB, USE_DRIVE, SEED, PROJECT_NAME)\n",
        "#       are defined in the Configuration cell above.\n",
        "\n",
        "# INSTALLATION\n",
        "!pip install -q medmnist wandb\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTS\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Deep Learning Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0, DenseNet121  # Lightweight models for small images\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# Metrics & Data\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, confusion_matrix)\n",
        "from sklearn.utils import class_weight\n",
        "from medmnist import DermaMNIST\n",
        "\n",
        "# ============================================================================\n",
        "# CONDITIONAL IMPORTS (based on configuration)\n",
        "\n",
        "# Weights & Biases\n",
        "if USE_WANDB:\n",
        "    try:\n",
        "        import wandb\n",
        "        from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "\n",
        "        # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        # ‚îÇ W&B LOGIN - Load API key from Colab Secrets                     ‚îÇ\n",
        "        # ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
        "            wandb.login(key=WANDB_API_KEY)\n",
        "            print(\"‚úÖ W&B logged in via Colab Secrets\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not auto-login to W&B: {e}\")\n",
        "            print(\"   You will be prompted to login on first run.\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è W&B not installed. Setting USE_WANDB = False\")\n",
        "        USE_WANDB = False\n",
        "        wandb = None\n",
        "else:\n",
        "    wandb = None\n",
        "    print(\"‚ÑπÔ∏è W&B tracking disabled (USE_WANDB = False)\")\n",
        "\n",
        "# Google Colab\n",
        "if USE_COLAB:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        print(\"‚úÖ Running in Google Colab\")\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è Not in Colab environment. Setting USE_COLAB = False\")\n",
        "        USE_COLAB = False\n",
        "\n",
        "# ============================================================================\n",
        "# REPRODUCIBILITY\n",
        "def set_seeds(seed=SEED):\n",
        "    \"\"\"Set random seeds for reproducibility across all libraries.\"\"\"\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_seeds()\n",
        "print(f\"‚úÖ Random seed set to {SEED}\")\n",
        "\n",
        "# ============================================================================\n",
        "# GPU CHECK\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"‚úÖ GPU detected: {gpus[0].name}\")\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NO GPU DETECTED - Training will be slow!\")\n",
        "    print(\"   Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "    print(\"   Local: Ensure CUDA and cuDNN are installed\")\n",
        "\n",
        "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STORAGE SETUP\n",
        "if USE_COLAB and USE_DRIVE:\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_PATH = f'/content/drive/MyDrive/{PROJECT_NAME}'\n",
        "    print(f\"‚úÖ Google Drive mounted\")\n",
        "else:\n",
        "    BASE_PATH = f'./{PROJECT_NAME}'\n",
        "    print(f\"‚ÑπÔ∏è Using local storage\")\n",
        "\n",
        "DIR_PHASE_1 = os.path.join(BASE_PATH, 'Phase1_Baselines')\n",
        "DIR_PHASE_2 = os.path.join(BASE_PATH, 'Phase2_Tuning')\n",
        "DIR_PHASE_3 = os.path.join(BASE_PATH, 'Phase3_Final')\n",
        "\n",
        "for folder in [DIR_PHASE_1, DIR_PHASE_2, DIR_PHASE_3]:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "print(f\"\\nüìÅ Project: {PROJECT_NAME}\")\n",
        "print(f\"üìÅ Storage paths:\")\n",
        "print(f\"   Phase 1: {DIR_PHASE_1}\")\n",
        "print(f\"   Phase 2: {DIR_PHASE_2}\")\n",
        "print(f\"   Phase 3: {DIR_PHASE_3}\")\n",
        "\n",
        "# ============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "def save_to_drive(experiment_name, target_folder):\n",
        "    \"\"\"\n",
        "    Save model checkpoint to the specified folder.\n",
        "\n",
        "    Args:\n",
        "        experiment_name (str): Name of the experiment (without .keras extension)\n",
        "        target_folder (str): Destination folder path\n",
        "    \"\"\"\n",
        "    filename = f\"{experiment_name}.keras\"\n",
        "    source = filename\n",
        "    destination = os.path.join(target_folder, filename)\n",
        "\n",
        "    if os.path.exists(source):\n",
        "        shutil.copy(source, destination)\n",
        "        print(f\"üíæ Model saved: {destination}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Model file not found: {source}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ SETUP COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "config_cell"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 1: Environment Setup\n",
        "\n",
        "Initializes the runtime environment: installs dependencies, configures GPU memory,\n",
        "sets random seeds for reproducibility, and creates storage directories for model checkpoints."
      ],
      "metadata": {
        "id": "E7NQ_f178GHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Note: Configuration variables (USE_COLAB, USE_WANDB, USE_DRIVE, SEED, PROJECT_NAME)\n",
        "#       are defined in the Configuration cell above.\n",
        "\n",
        "# INSTALLATION\n",
        "!pip install -q medmnist wandb\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTS\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Deep Learning Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0  # Lightweight models for small images\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# Metrics & Data\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, confusion_matrix)\n",
        "from sklearn.utils import class_weight\n",
        "from medmnist import DermaMNIST\n",
        "\n",
        "# ============================================================================\n",
        "# CONDITIONAL IMPORTS (based on configuration)\n",
        "\n",
        "# Weights & Biases\n",
        "if USE_WANDB:\n",
        "    try:\n",
        "        import wandb\n",
        "        from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "\n",
        "        # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        # ‚îÇ W&B LOGIN - Load API key from Colab Secrets                     ‚îÇ\n",
        "        # ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
        "            wandb.login(key=WANDB_API_KEY)\n",
        "            print(\"‚úÖ W&B logged in via Colab Secrets\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not auto-login to W&B: {e}\")\n",
        "            print(\"   You will be prompted to login on first run.\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è W&B not installed. Setting USE_WANDB = False\")\n",
        "        USE_WANDB = False\n",
        "        wandb = None\n",
        "else:\n",
        "    wandb = None\n",
        "    print(\"‚ÑπÔ∏è W&B tracking disabled (USE_WANDB = False)\")\n",
        "\n",
        "# Google Colab\n",
        "if USE_COLAB:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        print(\"‚úÖ Running in Google Colab\")\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è Not in Colab environment. Setting USE_COLAB = False\")\n",
        "        USE_COLAB = False\n",
        "\n",
        "# ============================================================================\n",
        "# REPRODUCIBILITY\n",
        "def set_seeds(seed=SEED):\n",
        "    \"\"\"Set random seeds for reproducibility across all libraries.\"\"\"\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_seeds()\n",
        "print(f\"‚úÖ Random seed set to {SEED}\")\n",
        "\n",
        "# ============================================================================\n",
        "# GPU CHECK\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"‚úÖ GPU detected: {gpus[0].name}\")\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NO GPU DETECTED - Training will be slow!\")\n",
        "    print(\"   Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "    print(\"   Local: Ensure CUDA and cuDNN are installed\")\n",
        "\n",
        "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STORAGE SETUP\n",
        "if USE_COLAB and USE_DRIVE:\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_PATH = f'/content/drive/MyDrive/{PROJECT_NAME}'\n",
        "    print(f\"‚úÖ Google Drive mounted\")\n",
        "else:\n",
        "    BASE_PATH = f'./{PROJECT_NAME}'\n",
        "    print(f\"‚ÑπÔ∏è Using local storage\")\n",
        "\n",
        "DIR_PHASE_1 = os.path.join(BASE_PATH, 'Phase1_Baselines')\n",
        "DIR_PHASE_2 = os.path.join(BASE_PATH, 'Phase2_Tuning')\n",
        "DIR_PHASE_3 = os.path.join(BASE_PATH, 'Phase3_Final')\n",
        "\n",
        "for folder in [DIR_PHASE_1, DIR_PHASE_2, DIR_PHASE_3]:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "print(f\"\\nüìÅ Project: {PROJECT_NAME}\")\n",
        "print(f\"üìÅ Storage paths:\")\n",
        "print(f\"   Phase 1: {DIR_PHASE_1}\")\n",
        "print(f\"   Phase 2: {DIR_PHASE_2}\")\n",
        "print(f\"   Phase 3: {DIR_PHASE_3}\")\n",
        "\n",
        "# ============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "def save_to_drive(experiment_name, target_folder):\n",
        "    \"\"\"\n",
        "    Save model checkpoint to the specified folder.\n",
        "\n",
        "    Args:\n",
        "        experiment_name (str): Name of the experiment (without .keras extension)\n",
        "        target_folder (str): Destination folder path\n",
        "    \"\"\"\n",
        "    filename = f\"{experiment_name}.keras\"\n",
        "    source = filename\n",
        "    destination = os.path.join(target_folder, filename)\n",
        "\n",
        "    if os.path.exists(source):\n",
        "        shutil.copy(source, destination)\n",
        "        print(f\"üíæ Model saved: {destination}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Model file not found: {source}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ SETUP COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "hpWpwubdlkv2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 2: Data Loading & Exploratory Data Analysis\n",
        "\n",
        "- **2a:** Explore raw data (7 original classes, pixel distributions)\n",
        "- **2b:** Transform to binary classification (Malignant vs Benign)\n",
        "- **2c:** Verify transformed data and compute class weights\n",
        "\n",
        "**Dataset:** DermaMNIST (10,015 dermatoscopic images, 28√ó28√ó3 RGB)\n",
        "\n",
        "**Binary Mapping:**\n",
        "- **Malignant (1):** akiec, bcc, mel (classes 0, 1, 6)\n",
        "- **Benign (0):** bkl, df, nv, vasc (classes 2, 3, 4, 5)"
      ],
      "metadata": {
        "id": "MCQ2a2RR8RJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 2a: EDA - RAW DATA EXPLORATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üìä EXPLORATORY DATA ANALYSIS - RAW DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "raw_train = DermaMNIST(split='train', download=True, size=28)\n",
        "raw_val = DermaMNIST(split='val', download=True, size=28)\n",
        "raw_test = DermaMNIST(split='test', download=True, size=28)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ DATASET OVERVIEW\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   Training samples:   {len(raw_train.imgs):,}\")\n",
        "print(f\"   Validation samples: {len(raw_val.imgs):,}\")\n",
        "print(f\"   Test samples:       {len(raw_test.imgs):,}\")\n",
        "print(f\"   Total samples:      {len(raw_train.imgs) + len(raw_val.imgs) + len(raw_test.imgs):,}\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ IMAGE PROPERTIES\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   Shape: {raw_train.imgs[0].shape}\")\n",
        "print(f\"   Dtype: {raw_train.imgs[0].dtype}\")\n",
        "print(f\"   Pixel range: [{raw_train.imgs.min()}, {raw_train.imgs.max()}]\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ ORIGINAL LABEL DISTRIBUTION (7 Classes)\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "class_names = {\n",
        "    0: 'Actinic keratoses (akiec)',      # Pre-cancerous\n",
        "    1: 'Basal cell carcinoma (bcc)',     # MALIGNANT - skin cancer\n",
        "    2: 'Benign keratosis (bkl)',         # Benign\n",
        "    3: 'Dermatofibroma (df)',            # Benign\n",
        "    4: 'Melanoma (mel)',                 # MALIGNANT - most dangerous skin cancer\n",
        "    5: 'Melanocytic nevi (nv)',          # Benign (common moles)\n",
        "    6: 'Vascular lesions (vasc)'         # Benign\n",
        "}\n",
        "\n",
        "# Count each class\n",
        "train_labels = raw_train.labels.flatten()\n",
        "unique, counts = np.unique(train_labels, return_counts=True)\n",
        "\n",
        "print(f\"   {'Class':<35} {'Count':>7} {'Percent':>8}\")\n",
        "print(f\"   {'-'*35} {'-'*7} {'-'*8}\")\n",
        "for cls, count in zip(unique, counts):\n",
        "    pct = 100 * count / len(train_labels)\n",
        "    print(f\"   {class_names[cls]:<35} {count:>7,} {pct:>7.1f}%\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart\n",
        "ax1 = axes[0]\n",
        "bars = ax1.bar(range(len(unique)), counts, color='steelblue', edgecolor='black')\n",
        "ax1.set_xticks(range(len(unique)))\n",
        "ax1.set_xticklabels([f\"{i}\\n{class_names[i].split('(')[1].replace(')', '')}\" for i in unique], fontsize=9)\n",
        "ax1.set_xlabel('Class')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.set_title('Original 7-Class Distribution (Training Set)')\n",
        "\n",
        "# Add count labels on bars\n",
        "for bar, count in zip(bars, counts):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
        "             f'{count:,}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Pie chart\n",
        "ax2 = axes[1]\n",
        "ax2.pie(counts, labels=[class_names[i].split('(')[1].replace(')', '') for i in unique],\n",
        "        autopct='%1.1f%%', startangle=90)\n",
        "ax2.set_title('Class Proportions (Training Set)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ SAMPLE IMAGES FROM EACH CLASS\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "fig, axes = plt.subplots(2, 7, figsize=(16, 5))\n",
        "\n",
        "for cls in range(7):\n",
        "    cls_indices = np.where(train_labels == cls)[0]\n",
        "\n",
        "    for row in range(2):\n",
        "        idx = cls_indices[row]\n",
        "        axes[row, cls].imshow(raw_train.imgs[idx])\n",
        "        axes[row, cls].axis('off')\n",
        "        if row == 0:\n",
        "            short_name = class_names[cls].split('(')[1].replace(')', '')\n",
        "            axes[row, cls].set_title(f'{short_name}\\n(Class {cls})', fontsize=9)\n",
        "\n",
        "plt.suptitle('Sample Images from Each Original Class', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n5Ô∏è‚É£ BINARY MAPPING PREVIEW\")\n",
        "print(\"-\"*40)\n",
        "print(\"   Malignant (1): Classes 0, 1, 4\")\n",
        "print(\"      - 0: Actinic keratoses (akiec)   ‚Üí Pre-cancerous\")\n",
        "print(\"      - 1: Basal cell carcinoma (bcc)  ‚Üí Skin cancer\")\n",
        "print(\"      - 4: Melanoma (mel)              ‚Üí Skin cancer (most dangerous)\")\n",
        "print(\"\\n   Benign (0): Classes 2, 3, 5, 6\")\n",
        "print(\"      - 2: Benign keratosis (bkl)      ‚Üí Benign growth\")\n",
        "print(\"      - 3: Dermatofibroma (df)         ‚Üí Benign fibrous nodule\")\n",
        "print(\"      - 5: Melanocytic nevi (nv)       ‚Üí Benign moles\")\n",
        "print(\"      - 6: Vascular lesions (vasc)     ‚Üí Benign vascular\")\n",
        "\n",
        "# Calculate expected binary distribution\n",
        "malignant_classes = [0, 1, 4]  # akiec, bcc, mel - CORRECTED!\n",
        "malignant_count = sum(counts[i] for i in range(len(unique)) if unique[i] in malignant_classes)\n",
        "benign_count = sum(counts[i] for i in range(len(unique)) if unique[i] not in malignant_classes)\n",
        "\n",
        "print(f\"\\n   Expected after transformation:\")\n",
        "print(f\"      Benign:    {benign_count:,} ({100*benign_count/len(train_labels):.1f}%)\")\n",
        "print(f\"      Malignant: {malignant_count:,} ({100*malignant_count/len(train_labels):.1f}%)\")\n",
        "print(f\"      Ratio:     {benign_count/malignant_count:.1f}:1 (Benign:Malignant)\")"
      ],
      "metadata": {
        "id": "eSPlDEB22QCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 2b: DATA TRANSFORMATION - BINARY CLASSIFICATION (CORRECTED)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üîÑ DATA TRANSFORMATION - BINARY CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def transform_to_binary(labels):\n",
        "    \"\"\"\n",
        "    Transform 7-class labels to binary (Benign=0, Malignant=1).\n",
        "\n",
        "    CORRECTED MAPPING:\n",
        "    - Malignant (1): akiec (0), bcc (1), mel (4)\n",
        "    - Benign (0): bkl (2), df (3), nv (5), vasc (6)\n",
        "\n",
        "    Args:\n",
        "        labels: Original 7-class labels (0-6)\n",
        "\n",
        "    Returns:\n",
        "        Binary labels (0=Benign, 1=Malignant)\n",
        "    \"\"\"\n",
        "    malignant_classes = [0, 1, 4]  # akiec, bcc, mel - CORRECTED!\n",
        "    binary_labels = np.isin(labels.flatten(), malignant_classes).astype(int)\n",
        "    return binary_labels\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ LOADING DATASETS...\")\n",
        "train_dataset = DermaMNIST(split='train', download=True, size=28)\n",
        "val_dataset = DermaMNIST(split='val', download=True, size=28)\n",
        "test_dataset = DermaMNIST(split='test', download=True, size=28)\n",
        "\n",
        "# Extract and normalize images\n",
        "print(\"2Ô∏è‚É£ NORMALIZING IMAGES (0-255 ‚Üí 0-1)...\")\n",
        "x_train = train_dataset.imgs.astype(np.float32) / 255.0\n",
        "x_val = val_dataset.imgs.astype(np.float32) / 255.0\n",
        "x_test = test_dataset.imgs.astype(np.float32) / 255.0\n",
        "\n",
        "# Transform labels to binary\n",
        "print(\"3Ô∏è‚É£ TRANSFORMING LABELS TO BINARY...\")\n",
        "y_train_binary = transform_to_binary(train_dataset.labels)\n",
        "y_val_binary = transform_to_binary(val_dataset.labels)\n",
        "y_test_binary = transform_to_binary(test_dataset.labels)\n",
        "\n",
        "# One-hot encode\n",
        "y_train = tf.keras.utils.to_categorical(y_train_binary, num_classes=2)\n",
        "y_val = tf.keras.utils.to_categorical(y_val_binary, num_classes=2)\n",
        "y_test = tf.keras.utils.to_categorical(y_test_binary, num_classes=2)\n",
        "\n",
        "# Verify binary distribution\n",
        "print(\"\\n4Ô∏è‚É£ BINARY CLASS DISTRIBUTION\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "for name, labels in [('Train', y_train_binary), ('Val', y_val_binary), ('Test', y_test_binary)]:\n",
        "    benign = np.sum(labels == 0)\n",
        "    malignant = np.sum(labels == 1)\n",
        "    total = len(labels)\n",
        "    print(f\"   {name:5}: Benign={benign:,} ({100*benign/total:.1f}%)  Malignant={malignant:,} ({100*malignant/total:.1f}%)\")\n",
        "\n",
        "# Calculate class imbalance ratio\n",
        "train_benign = np.sum(y_train_binary == 0)\n",
        "train_malignant = np.sum(y_train_binary == 1)\n",
        "imbalance_ratio = train_benign / train_malignant\n",
        "\n",
        "print(f\"\\n   Class Imbalance Ratio: {imbalance_ratio:.2f}:1 (Benign:Malignant)\")\n",
        "\n",
        "# Visualize binary distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "for ax, (name, labels) in zip(axes, [('Train', y_train_binary), ('Val', y_val_binary), ('Test', y_test_binary)]):\n",
        "    benign = np.sum(labels == 0)\n",
        "    malignant = np.sum(labels == 1)\n",
        "\n",
        "    bars = ax.bar(['Benign', 'Malignant'], [benign, malignant],\n",
        "                  color=['forestgreen', 'crimson'], edgecolor='black')\n",
        "    ax.set_title(f'{name} Set')\n",
        "    ax.set_ylabel('Count')\n",
        "\n",
        "    # Add count labels\n",
        "    for bar, count in zip(bars, [benign, malignant]):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20,\n",
        "                f'{count:,}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.suptitle('Binary Classification Distribution (CORRECTED)', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "data = {\n",
        "    'x_train': x_train,\n",
        "    'y_train': y_train,              # One-hot encoded [1,0] or [0,1]\n",
        "    'y_train_bin': y_train_binary,   # Binary: 0 or 1\n",
        "    'x_val': x_val,\n",
        "    'y_val': y_val,                  # One-hot encoded\n",
        "    'y_val_bin': y_val_binary,       # Binary: 0 or 1\n",
        "    'x_test': x_test,\n",
        "    'y_test': y_test,                # One-hot encoded\n",
        "    'y_test_bin': y_test_binary,     # Binary: 0 or 1\n",
        "    'class_weights': None            # Will be set in Block 6\n",
        "}\n",
        "\n",
        "print(\"\\n5Ô∏è‚É£ DATA SHAPES\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   x_train: {x_train.shape}  y_train: {y_train.shape}  y_train_bin: {y_train_binary.shape}\")\n",
        "print(f\"   x_val:   {x_val.shape}  y_val:   {y_val.shape}  y_val_bin:   {y_val_binary.shape}\")\n",
        "print(f\"   x_test:  {x_test.shape}  y_test:  {y_test.shape}  y_test_bin:  {y_test_binary.shape}\")\n",
        "\n",
        "# Data augmentation\n",
        "print(\"\\n6Ô∏è‚É£ DATA AUGMENTATION\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "print(\"   ‚úÖ ImageDataGenerator configured\")\n",
        "print(\"   Augmentations: rotation, shift, shear, zoom, flip\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ DATA TRANSFORMATION COMPLETE (CORRECTED LABELS)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\"\"\n",
        "   Binary Classes:\n",
        "   ‚îú‚îÄ‚îÄ 0 = Benign    (bkl, df, nv, vasc)\n",
        "   ‚îî‚îÄ‚îÄ 1 = Malignant (akiec, bcc, mel)\n",
        "\n",
        "   Class Imbalance: {imbalance_ratio:.1f}:1\n",
        "   ‚Üí Recommend using class weights (W3 or W5)\n",
        "\n",
        "   Data Dictionary Keys:\n",
        "   ‚îú‚îÄ‚îÄ x_train, x_val, x_test     (images)\n",
        "   ‚îú‚îÄ‚îÄ y_train, y_val, y_test     (one-hot labels)\n",
        "   ‚îî‚îÄ‚îÄ y_train_bin, y_val_bin, y_test_bin (binary labels)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "5r1UECKr2SPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 2c: EDA - TRANSFORMED DATA & DEFINE TRAIN, VAL & TEST DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä EXPLORATORY DATA ANALYSIS - TRANSFORMED DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ TRANSFORMED DATA OVERVIEW\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   {'Split':<12} {'Shape':<20} {'Dtype':<12} {'Pixel Range'}\")\n",
        "print(f\"   {'-'*12} {'-'*20} {'-'*12} {'-'*15}\")\n",
        "print(f\"   {'Train':<12} {str(data['x_train'].shape):<20} {str(data['x_train'].dtype):<12} [{data['x_train'].min():.2f}, {data['x_train'].max():.2f}]\")\n",
        "print(f\"   {'Val':<12} {str(data['x_val'].shape):<20} {str(data['x_val'].dtype):<12} [{data['x_val'].min():.2f}, {data['x_val'].max():.2f}]\")\n",
        "print(f\"   {'Test':<12} {str(data['x_test'].shape):<20} {str(data['x_test'].dtype):<12} [{data['x_test'].min():.2f}, {data['x_test'].max():.2f}]\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ BINARY LABEL DISTRIBUTION\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "for split_name, y_bin in [('Train', data['y_train_bin']),\n",
        "                           ('Val', data['y_val_bin']),\n",
        "                           ('Test', data['y_test_bin'])]:\n",
        "    total = len(y_bin)\n",
        "    malignant = y_bin.sum()\n",
        "    benign = total - malignant\n",
        "    print(f\"   {split_name}:\")\n",
        "    print(f\"      Benign (0):    {benign:>5,} ({100*benign/total:>5.1f}%)\")\n",
        "    print(f\"      Malignant (1): {int(malignant):>5,} ({100*malignant/total:>5.1f}%)\")\n",
        "    print(f\"      Ratio: {benign/malignant:.1f}:1\")\n",
        "    print()\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "for ax, (split_name, y_bin) in zip(axes, [('Train', data['y_train_bin']),\n",
        "                                           ('Val', data['y_val_bin']),\n",
        "                                           ('Test', data['y_test_bin'])]):\n",
        "    total = len(y_bin)\n",
        "    malignant = int(y_bin.sum())\n",
        "    benign = total - malignant\n",
        "\n",
        "    bars = ax.bar(['Benign\\n(0)', 'Malignant\\n(1)'], [benign, malignant],\n",
        "                  color=['forestgreen', 'crimson'], edgecolor='black')\n",
        "    ax.set_title(f'{split_name} Set (n={total:,})')\n",
        "    ax.set_ylabel('Count')\n",
        "\n",
        "    # Add labels\n",
        "    for bar, count in zip(bars, [benign, malignant]):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
        "                f'{count:,}\\n({100*count/total:.1f}%)', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.suptitle('Binary Class Distribution After Transformation', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ SAMPLE IMAGES BY BINARY CLASS\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "fig, axes = plt.subplots(2, 6, figsize=(14, 5))\n",
        "\n",
        "y_train_flat = data['y_train_bin'].flatten()\n",
        "\n",
        "for row, (cls, cls_name, color) in enumerate([(0, 'Benign', 'forestgreen'),\n",
        "                                                (1, 'Malignant', 'crimson')]):\n",
        "    cls_indices = np.where(y_train_flat == cls)[0]\n",
        "\n",
        "    for col in range(6):\n",
        "        idx = cls_indices[col]\n",
        "        axes[row, col].imshow(data['x_train'][idx])\n",
        "        axes[row, col].axis('off')\n",
        "        if col == 0:\n",
        "            axes[row, col].set_ylabel(cls_name, fontsize=12, color=color, fontweight='bold')\n",
        "\n",
        "axes[0, 0].set_title('Benign Samples (Class 0)', fontsize=10, loc='left', color='forestgreen')\n",
        "axes[1, 0].set_title('Malignant Samples (Class 1)', fontsize=10, loc='left', color='crimson')\n",
        "\n",
        "plt.suptitle('Sample Images After Binary Transformation', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ ONE-HOT ENCODING VERIFICATION\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   y_train shape: {data['y_train'].shape}\")\n",
        "print(f\"   y_train sample (first 5):\")\n",
        "print(f\"   {data['y_train'][:5]}\")\n",
        "\n",
        "print(\"\\n5Ô∏è‚É£ DATA AUGMENTATION PREVIEW\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "# Pick one sample\n",
        "sample_idx = np.where(data['y_train_bin'].flatten() == 1)[0][0]  # First malignant\n",
        "sample_img = data['x_train'][sample_idx:sample_idx+1]\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "\n",
        "# Original\n",
        "axes[0, 0].imshow(sample_img[0])\n",
        "axes[0, 0].set_title('Original', fontweight='bold')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "# Augmented versions\n",
        "aug_iter = datagen.flow(sample_img, batch_size=1, seed=SEED)\n",
        "for i in range(1, 10):\n",
        "    row = i // 5\n",
        "    col = i % 5\n",
        "    aug_img = next(aug_iter)[0]\n",
        "    axes[row, col].imshow(aug_img)\n",
        "    axes[row, col].set_title(f'Augmented {i}')\n",
        "    axes[row, col].axis('off')\n",
        "\n",
        "plt.suptitle('Data Augmentation Examples (Same Image)', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n6Ô∏è‚É£ CLASS IMBALANCE SUMMARY\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "# Calculate actual ratio from data\n",
        "train_mal = data['y_train_bin'].sum()\n",
        "train_ben = len(data['y_train_bin']) - train_mal\n",
        "actual_ratio = train_ben / train_mal\n",
        "\n",
        "print(f\"   ‚ö†Ô∏è  Your data is IMBALANCED\")\n",
        "print(f\"   \")\n",
        "print(f\"   Benign:Malignant ratio = ~{actual_ratio:.0f}:1\")\n",
        "print(f\"   \")\n",
        "print(f\"   Without intervention, model will likely:\")\n",
        "print(f\"      ‚Ä¢ Predict 'Benign' most of the time ({100*train_ben/len(data['y_train_bin']):.0f}% accuracy!)\")\n",
        "print(f\"      ‚Ä¢ Have LOW recall for Malignant class\")\n",
        "print(f\"   \")\n",
        "print(f\"   Strategies to address:\")\n",
        "print(f\"      ‚Ä¢ Class weights\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ EDA COMPLETE - Ready for modeling\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jnR9QMmx2WY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 3: Model Architectures\n",
        "\n",
        "## 1. Custom CNN (`build_custom_cnn`)\n",
        "Lightweight CNN optimized for native 28√ó28 resolution with VGG-style double convolutions.\n",
        "\n",
        "| Parameter | Description |\n",
        "|-----------|-------------|\n",
        "| `filters_base` | Initial filters (doubles per block) |\n",
        "| `depth` | Number of conv blocks (2-4) |\n",
        "| `dropout` | Dropout rate |\n",
        "| `dense_units` | Dense layer size |\n",
        "\n",
        "## 2. Hybrid CNN with CBAM (`build_hybrid_cnn`)\n",
        "CNN with Convolutional Block Attention Module for channel and spatial attention.\n",
        "\n",
        "## 3. Transfer Learning (`build_transfer_model`)\n",
        "Pre-trained models with custom classification head:\n",
        "- **MobileNetV2** (3.4M params) - Lightweight, min input 32√ó32\n",
        "- **EfficientNetB0** (5.3M params) - Compound scaling\n",
        "- **DenseNet121** (8M params) - Dense connections, better gradient flow\n",
        "\n",
        "All transfer models upsample 28√ó28 ‚Üí 56√ó56 using bilinear interpolation."
      ],
      "metadata": {
        "id": "H8uMQLMq8eaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ============================================================================\n",
        "# BLOCK 3: MODEL ARCHITECTURES\n",
        "# ============================================================================\n",
        "#   1. Custom CNN        - Optimized baseline CNN (VGG-style double conv)\n",
        "#   2. Hybrid CNN (CBAM) - CNN with Channel & Spatial Attention (like MedNet)\n",
        "#   3. Transfer Learning - MobileNetV2 & EfficientNetB0 (lightweight, for small images)\n",
        "\n",
        "def build_custom_cnn(filters_base=32, depth=3, dropout=0.5, dense_units=256):\n",
        "    \"\"\"\n",
        "    Optimized CNN baseline for 28x28 images.\n",
        "\n",
        "    Improvements over naive CNN:\n",
        "    - Double Conv per block (VGG-style) for better feature extraction\n",
        "    - GlobalAveragePooling instead of Flatten (fewer params, more robust)\n",
        "    - Constant dropout (not increasing) to avoid over-regularization\n",
        "    - Filter cap at 4x base to prevent parameter explosion\n",
        "\n",
        "    Args:\n",
        "        filters_base: Starting number of filters (32 or 64 recommended)\n",
        "        depth: Number of Conv blocks (3-4 recommended for 28x28)\n",
        "        dropout: Dropout rate (0.4-0.5 recommended)\n",
        "        dense_units: Neurons in dense layer (128-256 recommended)\n",
        "\n",
        "    Returns:\n",
        "        Keras Sequential model\n",
        "    \"\"\"\n",
        "    model = models.Sequential([layers.Input(shape=(28, 28, 3))])\n",
        "\n",
        "    for i in range(depth):\n",
        "        # Cap filters at 4x base (e.g., 32‚Üí64‚Üí128‚Üí128, not 32‚Üí64‚Üí128‚Üí256‚Üí512)\n",
        "        filters = filters_base * (2 ** min(i, 2))\n",
        "\n",
        "        # Double Conv per block (VGG-style) - better feature extraction\n",
        "        model.add(layers.Conv2D(filters, (3, 3), padding='same'))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Activation('relu'))\n",
        "\n",
        "        model.add(layers.Conv2D(filters, (3, 3), padding='same'))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Activation('relu'))\n",
        "\n",
        "        model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "        # Constant dropout per block (not increasing!)\n",
        "        model.add(layers.Dropout(dropout * 0.5))\n",
        "\n",
        "    # GlobalAveragePooling instead of Flatten\n",
        "    # - Fewer parameters\n",
        "    # - More robust to spatial variations\n",
        "    # - Works better with small images\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "    # Classification head\n",
        "    model.add(layers.Dense(dense_units, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(dropout))\n",
        "    model.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def channel_attention(input_tensor, ratio=8):\n",
        "    \"\"\"\n",
        "    Channel Attention Module.\n",
        "    Focuses on 'what' is meaningful in the feature maps.\n",
        "\n",
        "    Args:\n",
        "        input_tensor: Input feature map\n",
        "        ratio: Reduction ratio for the bottleneck\n",
        "\n",
        "    Returns:\n",
        "        Attention-weighted feature map\n",
        "    \"\"\"\n",
        "    channels = input_tensor.shape[-1]\n",
        "\n",
        "    # Global Average Pooling\n",
        "    avg_pool = layers.GlobalAveragePooling2D()(input_tensor)\n",
        "    avg_pool = layers.Reshape((1, 1, channels))(avg_pool)\n",
        "\n",
        "    # Global Max Pooling\n",
        "    max_pool = layers.GlobalMaxPooling2D()(input_tensor)\n",
        "    max_pool = layers.Reshape((1, 1, channels))(max_pool)\n",
        "\n",
        "    # Shared MLP\n",
        "    shared_dense1 = layers.Dense(channels // ratio, activation='relu')\n",
        "    shared_dense2 = layers.Dense(channels, activation='linear')\n",
        "\n",
        "    avg_out = shared_dense2(shared_dense1(avg_pool))\n",
        "    max_out = shared_dense2(shared_dense1(max_pool))\n",
        "\n",
        "    # Combine and apply sigmoid\n",
        "    attention = layers.Add()([avg_out, max_out])\n",
        "    attention = layers.Activation('sigmoid')(attention)\n",
        "\n",
        "    return layers.Multiply()([input_tensor, attention])\n",
        "\n",
        "def spatial_attention(input_tensor, kernel_size=7):\n",
        "    \"\"\"\n",
        "    Spatial Attention Module.\n",
        "    Focuses on 'where' is meaningful in the feature maps.\n",
        "\n",
        "    Args:\n",
        "        input_tensor: Input feature map\n",
        "        kernel_size: Size of convolution kernel\n",
        "\n",
        "    Returns:\n",
        "        Attention-weighted feature map\n",
        "    \"\"\"\n",
        "    # Average and Max along channel axis\n",
        "    avg_pool = layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(input_tensor)\n",
        "    max_pool = layers.Lambda(lambda x: tf.reduce_max(x, axis=-1, keepdims=True))(input_tensor)\n",
        "\n",
        "    # Concatenate\n",
        "    concat = layers.Concatenate(axis=-1)([avg_pool, max_pool])\n",
        "\n",
        "    # Convolution\n",
        "    attention = layers.Conv2D(1, kernel_size, padding='same', activation='sigmoid')(concat)\n",
        "\n",
        "    return layers.Multiply()([input_tensor, attention])\n",
        "\n",
        "def cbam_block(input_tensor, ratio=8, kernel_size=7):\n",
        "    \"\"\"\n",
        "    CBAM: Convolutional Block Attention Module.\n",
        "    Combines Channel and Spatial Attention sequentially.\n",
        "\n",
        "    Reference: \"CBAM: Convolutional Block Attention Module\" (Woo et al., 2018)\n",
        "\n",
        "    Args:\n",
        "        input_tensor: Input feature map\n",
        "        ratio: Channel attention reduction ratio\n",
        "        kernel_size: Spatial attention kernel size\n",
        "\n",
        "    Returns:\n",
        "        Attention-refined feature map\n",
        "    \"\"\"\n",
        "    x = channel_attention(input_tensor, ratio)\n",
        "    x = spatial_attention(x, kernel_size)\n",
        "    return x\n",
        "\n",
        "def build_hybrid_cnn(filters_base=64, depth=3, dropout=0.5, dense_units=256, use_cbam=True):\n",
        "    \"\"\"\n",
        "    Hybrid CNN with CBAM Attention - Similar to MedNet architecture.\n",
        "\n",
        "    This combines:\n",
        "    - Standard Convolutions (more stable than SeparableConv for small images)\n",
        "    - CBAM Attention (focuses on relevant features)\n",
        "    - Global Average Pooling (more robust than Flatten)\n",
        "\n",
        "    Inspired by MedNet which achieved 0.840 accuracy on DermaMNIST.\n",
        "\n",
        "    Args:\n",
        "        filters_base: Starting filters (64 recommended)\n",
        "        depth: Number of blocks (3 recommended for 28x28)\n",
        "        dropout: Dropout rate (0.5 recommended)\n",
        "        dense_units: Dense layer neurons (256 recommended)\n",
        "        use_cbam: Whether to use CBAM attention\n",
        "\n",
        "    Returns:\n",
        "        Keras Functional model\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=(28, 28, 3))\n",
        "    x = inputs\n",
        "\n",
        "    # Initial convolution\n",
        "    x = layers.Conv2D(filters_base, (3, 3), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    for i in range(depth):\n",
        "        # Cap filters at 4x base (256 max for filters_base=64)\n",
        "        filters = filters_base * (2 ** min(i, 2))\n",
        "\n",
        "        # Double Conv per block (VGG-style) - more stable than SeparableConv\n",
        "        x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('relu')(x)\n",
        "\n",
        "        x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('relu')(x)\n",
        "\n",
        "        # CBAM Attention after each block (only if feature map large enough)\n",
        "        if use_cbam and x.shape[1] >= 7:  # CBAM needs at least 7x7 for kernel_size=7\n",
        "            x = cbam_block(x, ratio=8, kernel_size=3)  # Smaller kernel for small feature maps\n",
        "\n",
        "        # MaxPooling\n",
        "        x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "        # Constant dropout (not increasing)\n",
        "        x = layers.Dropout(dropout * 0.5)(x)\n",
        "\n",
        "    # Global pooling instead of flatten (more robust, fewer parameters)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Classification head\n",
        "    x = layers.Dense(dense_units, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(2, activation='softmax')(x)\n",
        "\n",
        "    model_name = \"hybrid_cnn_cbam\" if use_cbam else \"hybrid_cnn\"\n",
        "    return models.Model(inputs, outputs, name=model_name)\n",
        "\n",
        "def build_transfer_model(base_name='mobilenetv2', dropout=0.5, unfreeze_layers=None, dense_units=256):\n",
        "    \"\"\"\n",
        "    Transfer Learning with lightweight models optimized for small images.\n",
        "\n",
        "    Available Models:\n",
        "    - MobileNetV2:    3.4M params, min input 32x32, designed for mobile/edge\n",
        "    - EfficientNetB0: 5.3M params, efficient compound scaling\n",
        "    - DenseNet121:    8M params, dense connections (better for small feature maps)\n",
        "\n",
        "    Args:\n",
        "        base_name: 'mobilenetv2', 'efficientnet', or 'densenet'\n",
        "        dropout: Dropout rate\n",
        "        unfreeze_layers: Fine-tuning strategy\n",
        "            - None: Full fine-tuning (all layers trainable)\n",
        "            - 0: Feature extraction only (all layers frozen)\n",
        "            - N: Unfreeze last N layers\n",
        "        dense_units: Dense layer neurons\n",
        "\n",
        "    Returns:\n",
        "        Keras Functional model\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=(28, 28, 3))\n",
        "\n",
        "    # Upsampling: 28x28 ‚Üí 56x56 (minimum for decent feature extraction)\n",
        "    # Note: These models work best at 224x224, but 56x56 is a compromise\n",
        "    x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(inputs)\n",
        "\n",
        "    # Available base models (lightweight, suitable for small images)\n",
        "    base_models = {\n",
        "        'mobilenetv2': lambda: MobileNetV2(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(56, 56, 3),\n",
        "            pooling='avg'\n",
        "        ),\n",
        "        'efficientnet': lambda: EfficientNetB0(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(56, 56, 3),\n",
        "            pooling='avg'\n",
        "        ),\n",
        "        'densenet': lambda: DenseNet121(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(56, 56, 3),\n",
        "            pooling='avg'\n",
        "        )\n",
        "    }\n",
        "\n",
        "    if base_name not in base_models:\n",
        "        raise ValueError(f\"Unknown base: {base_name}. Choose from: {list(base_models.keys())}\")\n",
        "\n",
        "    base = base_models[base_name]()\n",
        "\n",
        "    print(f\"   {base_name}: {len(base.layers)} total layers\")\n",
        "\n",
        "    # Fine-tuning strategy\n",
        "    if unfreeze_layers is None:\n",
        "        # Full fine-tuning - all layers trainable\n",
        "        base.trainable = True\n",
        "        print(f\"   Mode: Full fine-tuning (all {len(base.layers)} layers trainable)\")\n",
        "    elif unfreeze_layers == 0:\n",
        "        # Feature extraction - all layers frozen\n",
        "        base.trainable = False\n",
        "        print(f\"   Mode: Feature extraction (all layers frozen)\")\n",
        "    else:\n",
        "        # Partial fine-tuning - freeze early layers, train later ones\n",
        "        base.trainable = True\n",
        "        for layer in base.layers[:-unfreeze_layers]:\n",
        "            layer.trainable = False\n",
        "        trainable_count = sum([1 for l in base.layers if l.trainable])\n",
        "        print(f\"   Mode: Partial fine-tuning ({trainable_count}/{len(base.layers)} layers trainable)\")\n",
        "\n",
        "    # Forward pass through base model (training=False for BatchNorm layers)\n",
        "    x = base(x, training=False)\n",
        "\n",
        "    # Classification head\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(dense_units, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout * 0.5)(x)\n",
        "    outputs = layers.Dense(2, activation='softmax')(x)\n",
        "\n",
        "    return models.Model(inputs, outputs, name=f\"{base_name}_model\")\n",
        "\n",
        "# SUMMARY\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ BLOCK 3: MODEL ARCHITECTURES LOADED\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\"\"\n",
        "\n",
        "   Key Improvements in Custom CNN:\n",
        "   ‚îú‚îÄ‚îÄ Double Conv per block (VGG-style)\n",
        "   ‚îú‚îÄ‚îÄ GlobalAveragePooling (not Flatten)\n",
        "   ‚îú‚îÄ‚îÄ Constant dropout (not increasing)\n",
        "   ‚îî‚îÄ‚îÄ Filter cap at 4x base\n",
        "\n",
        "   DenseNet121 Advantages:\n",
        "   ‚îú‚îÄ‚îÄ Dense connections ‚Üí better gradient flow\n",
        "   ‚îú‚îÄ‚îÄ Feature reuse ‚Üí efficient for small images\n",
        "   ‚îî‚îÄ‚îÄ Min input 32√ó32 ‚Üí works well with 56√ó56 upsampling\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "bQAIvl1XlpL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 4: Training Configuration & Helpers\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### `compile_model(model, learning_rate)`\n",
        "Compiles with Adam optimizer, categorical crossentropy, and clinical metrics.\n",
        "\n",
        "### `get_callbacks(config, model_path)`\n",
        "Creates callbacks with **F2 Score** monitoring (not validation loss).\n",
        "\n",
        "**Why F2 Score?**\n",
        "$$F_2 = \\frac{5 \\times \\text{Precision} \\times \\text{Recall}}{4 \\times \\text{Precision} + \\text{Recall}}$$\n",
        "\n",
        "- Weights Recall 2√ó more than Precision\n",
        "- Prevents \"all Malignant\" predictions (would have low precision ‚Üí low F2)\n",
        "- Prevents \"all Benign\" predictions (would have zero recall ‚Üí F2 = 0)\n",
        "- Aligns directly with clinical priority: **don't miss cancer!**\n",
        "\n",
        "| Callback | Monitor | Mode | Purpose |\n",
        "|----------|---------|------|---------|\n",
        "| EarlyStopping | val_f2 | max | Stop when F2 plateaus |\n",
        "| ModelCheckpoint | val_f2 | max | Save best F2 model |"
      ],
      "metadata": {
        "id": "KfeV-wQT8t83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 4: TRAINING CONFIGURATION & HELPERS (WITH F2 SCORE)\n",
        "# ============================================================================\n",
        "\n",
        "MONITOR_METRIC = 'val_f2_score'\n",
        "MONITOR_MODE = 'max'\n",
        "\n",
        "# F2 SCORE METRIC\n",
        "\n",
        "class F2Score(tf.keras.metrics.Metric):\n",
        "    \"\"\"\n",
        "    F2 Score metric that weights Recall 2x more than Precision.\n",
        "\n",
        "    F2 = 5 √ó (Precision √ó Recall) / (4 √ó Precision + Recall)\n",
        "\n",
        "    For binary classification with class_id=1 (Malignant).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name='f2_score', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
        "        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n",
        "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "        y_true_classes = tf.argmax(y_true, axis=1)\n",
        "\n",
        "        # Cast to float for calculations\n",
        "        y_pred_classes = tf.cast(y_pred_classes, tf.float32)\n",
        "        y_true_classes = tf.cast(y_true_classes, tf.float32)\n",
        "\n",
        "        # Calculate TP, FP, FN for class 1 (Malignant)\n",
        "        tp = tf.reduce_sum(y_true_classes * y_pred_classes)\n",
        "        fp = tf.reduce_sum((1 - y_true_classes) * y_pred_classes)\n",
        "        fn = tf.reduce_sum(y_true_classes * (1 - y_pred_classes))\n",
        "\n",
        "        self.true_positives.assign_add(tp)\n",
        "        self.false_positives.assign_add(fp)\n",
        "        self.false_negatives.assign_add(fn)\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n",
        "        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n",
        "\n",
        "        # F2 formula: (1 + beta^2) * (precision * recall) / (beta^2 * precision + recall)\n",
        "        # beta = 2 for F2\n",
        "        f2 = 5 * (precision * recall) / (4 * precision + recall + tf.keras.backend.epsilon())\n",
        "\n",
        "        return f2\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.true_positives.assign(0)\n",
        "        self.false_positives.assign(0)\n",
        "        self.false_negatives.assign(0)\n",
        "\n",
        "# W&B METRICS LOGGER (with Loss)\n",
        "\n",
        "class WandbGroupedMetricsLogger(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Logs metrics in grouped format for better W&B visualization.\n",
        "\n",
        "    Groups:\n",
        "    - loss/train, loss/val\n",
        "    - f2/train, f2/val\n",
        "    - recall/train, recall/val\n",
        "    - precision/train, precision/val\n",
        "    - auc/train, auc/val\n",
        "    \"\"\"\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if not USE_WANDB or wandb is None:\n",
        "            return\n",
        "\n",
        "        logs = logs or {}\n",
        "\n",
        "        grouped_logs = {\n",
        "            'epoch': epoch,\n",
        "            # Loss\n",
        "            'loss/train': logs.get('loss'),\n",
        "            'loss/val': logs.get('val_loss'),\n",
        "            # F2 Score (Early Stopping Metric)\n",
        "            'f2/train': logs.get('f2_score'),\n",
        "            'f2/val': logs.get('val_f2_score'),\n",
        "            # Recall (primary clinical metric)\n",
        "            'recall/train': logs.get('recall'),\n",
        "            'recall/val': logs.get('val_recall'),\n",
        "            # Precision\n",
        "            'precision/train': logs.get('precision'),\n",
        "            'precision/val': logs.get('val_precision'),\n",
        "            # AUC\n",
        "            'auc/train': logs.get('auc'),\n",
        "            'auc/val': logs.get('val_auc'),\n",
        "        }\n",
        "\n",
        "        # Remove None values\n",
        "        grouped_logs = {k: v for k, v in grouped_logs.items() if v is not None}\n",
        "\n",
        "        wandb.log(grouped_logs)\n",
        "\n",
        "# MODEL COMPILATION\n",
        "\n",
        "def compile_model(model, learning_rate):\n",
        "    \"\"\"\n",
        "    Compile model with all metrics including F2 Score.\n",
        "\n",
        "    Metrics:\n",
        "    - loss (categorical_crossentropy)\n",
        "    - auc\n",
        "    - precision (for Malignant class)\n",
        "    - recall (for Malignant class)\n",
        "    - f2_score (custom)\n",
        "    \"\"\"\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=[\n",
        "            tf.keras.metrics.AUC(name='auc'),\n",
        "            tf.keras.metrics.Precision(class_id=1, name='precision'),\n",
        "            tf.keras.metrics.Recall(class_id=1, name='recall'),\n",
        "            F2Score(name='f2_score')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# SUMMARY\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ BLOCK 4: TRAINING CONFIGURATION LOADED\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\"\"\n",
        "   Monitoring Metric: {MONITOR_METRIC} ({MONITOR_MODE})\n",
        "\n",
        "   F2 Score Formula:\n",
        "   F2 = 5 √ó (Precision √ó Recall) / (4 √ó Precision + Recall)\n",
        "\n",
        "   Why F2?\n",
        "   ‚îú‚îÄ‚îÄ Weights Recall 2x more than Precision\n",
        "   ‚îú‚îÄ‚îÄ Ideal for medical diagnosis (don't miss cancer!)\n",
        "   ‚îú‚îÄ‚îÄ Prevents \"all Malignant\" models (Precision too low)\n",
        "   ‚îî‚îÄ‚îÄ Prevents \"all Benign\" models (Recall too low)\n",
        "\n",
        "   Metrics logged to W&B:\n",
        "   ‚îú‚îÄ‚îÄ loss/train, loss/val\n",
        "   ‚îú‚îÄ‚îÄ f2/train, f2/val\n",
        "   ‚îú‚îÄ‚îÄ recall/train, recall/val\n",
        "   ‚îú‚îÄ‚îÄ precision/train, precision/val\n",
        "   ‚îî‚îÄ‚îÄ auc/train, auc/val\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "aHM5rFQrlrAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 5: Main Training Loop\n",
        "\n",
        "## `train_experiment(config, data, datagen)`\n",
        "\n",
        "Orchestrates the complete training pipeline for any architecture.\n",
        "\n",
        "### Pipeline Steps\n",
        "1. **Initialize W&B** ‚Äî Start experiment tracking\n",
        "2. **Build Model** ‚Äî Custom CNN, Hybrid CNN, or Transfer Learning\n",
        "3. **Create Generator** ‚Äî Infinite augmented data stream\n",
        "4. **Train** ‚Äî Fit with callbacks, class weights, and F2 monitoring\n",
        "5. **Find Best Epoch** ‚Äî Select epoch with highest validation F2\n",
        "6. **Confusion Matrix** ‚Äî Compute TP/FP/TN/FN at best epoch\n",
        "7. **Log Results** ‚Äî Save all metrics to W&B\n",
        "\n",
        "### Returns\n",
        "Dictionary with best epoch metrics: F2, Recall, Precision, AUC, Confusion Matrix"
      ],
      "metadata": {
        "id": "ehLF6x378rdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BLOCK 5: TRAINING LOOP WITH F2-BASED EARLY STOPPING\n",
        "# ============================================================================\n",
        "# This block contains the main training function that:\n",
        "#   - Builds the appropriate model based on config['architecture']\n",
        "#   - Trains with F2-Score monitoring\n",
        "#   - Logs to W&B\n",
        "#   - Returns results dictionary\n",
        "\n",
        "def train_experiment(config, data, datagen):\n",
        "    \"\"\"\n",
        "    Train a single experiment with the given configuration.\n",
        "\n",
        "    Supports architectures:\n",
        "        - 'custom_cnn': build_custom_cnn()\n",
        "        - 'hybrid_cnn': build_hybrid_cnn()\n",
        "        - 'transfer':   build_transfer_model()\n",
        "\n",
        "    Args:\n",
        "        config: Dictionary with model configuration\n",
        "        data: Dictionary with train/val/test data and class weights\n",
        "        datagen: ImageDataGenerator for augmentation\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with training results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"üöÄ STARTING: {config['name']}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # 1. BUILD MODEL based on architecture type\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    arch = config['architecture']\n",
        "\n",
        "    if arch == 'custom_cnn':\n",
        "        model = build_custom_cnn(\n",
        "            filters_base=config.get('filters_base', 32),\n",
        "            depth=config.get('depth', 3),\n",
        "            dropout=config.get('dropout', 0.5),\n",
        "            dense_units=config.get('dense_units', 512)\n",
        "        )\n",
        "\n",
        "    elif arch == 'hybrid_cnn':\n",
        "        model = build_hybrid_cnn(\n",
        "            filters_base=config.get('filters_base', 64),\n",
        "            depth=config.get('depth', 4),\n",
        "            dropout=config.get('dropout', 0.5),\n",
        "            dense_units=config.get('dense_units', 256),\n",
        "            use_cbam=config.get('use_cbam', True)\n",
        "        )\n",
        "\n",
        "    elif arch == 'transfer':\n",
        "        model = build_transfer_model(\n",
        "            base_name=config.get('base_name', 'mobilenetv2'),\n",
        "            dropout=config.get('dropout', 0.5),\n",
        "            unfreeze_layers=config.get('unfreeze_layers', None),\n",
        "            dense_units=config.get('dense_units', 256)\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown architecture: {arch}\")\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # 2. COMPILE MODEL with F2 metric\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # Note: compile_model() modifies model in-place, doesn't return anything\n",
        "    compile_model(model, learning_rate=config.get('learning_rate', 0.001))\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # 3. SETUP CALLBACKS\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    callback_list = [\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor=MONITOR_METRIC,\n",
        "            patience=config.get('patience', 10),\n",
        "            mode=MONITOR_MODE,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        callbacks.ModelCheckpoint(\n",
        "            filepath=f\"{config['name']}.keras\",\n",
        "            monitor=MONITOR_METRIC,\n",
        "            mode=MONITOR_MODE,\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Add W&B callback if enabled\n",
        "    if USE_WANDB and wandb is not None:\n",
        "        run = wandb.init(\n",
        "            project=PROJECT_NAME,\n",
        "            name=config['name'],\n",
        "            config=config,\n",
        "            reinit=True\n",
        "        )\n",
        "        callback_list.append(WandbGroupedMetricsLogger())\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # 4. PREPARE DATA\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    batch_size = config.get('batch_size', 64)\n",
        "\n",
        "    train_generator = datagen.flow(\n",
        "        data['x_train'], data['y_train'],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Validation data (no augmentation)\n",
        "    val_data = (data['x_val'], data['y_val'])\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # 5. TRAIN MODEL\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=config.get('epochs', 50),\n",
        "        validation_data=val_data,\n",
        "        class_weight=data['class_weights'],\n",
        "        callbacks=callback_list,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # 6. EVALUATE BEST MODEL\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    print(\"\\nüìä Generating Confusion Matrix for BEST Model...\")\n",
        "\n",
        "    # Find best epoch by F2\n",
        "    val_f2_history = history.history['val_f2_score']\n",
        "    best_epoch_idx = np.argmax(val_f2_history)\n",
        "\n",
        "    # Get metrics at best epoch\n",
        "    best_val_f2 = val_f2_history[best_epoch_idx]\n",
        "    best_val_recall = history.history['val_recall'][best_epoch_idx]\n",
        "    best_val_precision = history.history['val_precision'][best_epoch_idx]\n",
        "    best_val_auc = history.history['val_auc'][best_epoch_idx]\n",
        "\n",
        "    # Generate predictions on validation set\n",
        "    y_pred_proba = model.predict(data['x_val'], verbose=0)\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "    y_true = np.argmax(data['y_val'], axis=1)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # Calculate F2 from confusion matrix for verification\n",
        "    precision_cm = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall_cm = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f2_cm = 5 * (precision_cm * recall_cm) / (4 * precision_cm + recall_cm) if (4 * precision_cm + recall_cm) > 0 else 0\n",
        "\n",
        "    print(f\"\"\"\n",
        "   Confusion Matrix:\n",
        "                    Predicted\n",
        "                  Benign  Malignant\n",
        "   Actual Benign   (TN) {tn:4d}   (FP) {fp:4d}\n",
        "          Malig.   (FN) {fn:4d}   (TP) {tp:4d}\n",
        "\n",
        "   ‚ùå Missed Cancers (FN): {fn}\n",
        "   ‚ö†Ô∏è  False Alarms (FP):  {fp}\"\"\")\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # 7. SAVE CONFUSION MATRIX PLOT\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    save_path = f\"cm_{config['name']}.png\"\n",
        "    plot_confusion_matrix(cm, config['name'], save_path,\n",
        "                          recall=best_val_recall,\n",
        "                          precision=best_val_precision,\n",
        "                          f2=best_val_f2)\n",
        "    print(f\"üìä Confusion matrix saved: {save_path}\")\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # 8. LOG BEST MODEL METRICS TO W&B SUMMARY\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    if USE_WANDB and wandb is not None:\n",
        "        # Use wandb.summary for final/best metrics (not wandb.log!)\n",
        "        # These appear in the W&B Table for easy model comparison\n",
        "        wandb.summary['best_val_f2'] = best_val_f2\n",
        "        wandb.summary['best_val_recall'] = best_val_recall\n",
        "        wandb.summary['best_val_precision'] = best_val_precision\n",
        "        wandb.summary['best_val_auc'] = best_val_auc\n",
        "        wandb.summary['best_val_loss'] = history.history['val_loss'][best_epoch_idx]\n",
        "        wandb.summary['best_epoch'] = best_epoch_idx + 1\n",
        "        wandb.summary['total_epochs'] = len(history.history['loss'])\n",
        "\n",
        "        # Confusion matrix values\n",
        "        wandb.summary['cm_tn'] = tn\n",
        "        wandb.summary['cm_fp'] = fp\n",
        "        wandb.summary['cm_fn'] = fn\n",
        "        wandb.summary['cm_tp'] = tp\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # VISUALIZE BEST EPOCH IN TRAINING CURVES\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # Create a custom chart showing best epoch marker\n",
        "        epochs_range = list(range(1, len(history.history['val_f2_score']) + 1))\n",
        "\n",
        "        # Create training curves plot with best epoch highlighted\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "        # F2 Score\n",
        "        axes[0, 0].plot(epochs_range, history.history['f2_score'], 'b-', label='Train F2')\n",
        "        axes[0, 0].plot(epochs_range, history.history['val_f2_score'], 'r-', label='Val F2')\n",
        "        axes[0, 0].axvline(x=best_epoch_idx + 1, color='g', linestyle='--', label=f'Best Epoch ({best_epoch_idx + 1})')\n",
        "        axes[0, 0].scatter([best_epoch_idx + 1], [best_val_f2], color='g', s=100, zorder=5)\n",
        "        axes[0, 0].set_title(f'F2 Score (Best: {best_val_f2:.4f})')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss\n",
        "        axes[0, 1].plot(epochs_range, history.history['loss'], 'b-', label='Train Loss')\n",
        "        axes[0, 1].plot(epochs_range, history.history['val_loss'], 'r-', label='Val Loss')\n",
        "        axes[0, 1].axvline(x=best_epoch_idx + 1, color='g', linestyle='--', label=f'Best Epoch ({best_epoch_idx + 1})')\n",
        "        axes[0, 1].set_title('Loss')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Recall\n",
        "        axes[1, 0].plot(epochs_range, history.history['recall'], 'b-', label='Train Recall')\n",
        "        axes[1, 0].plot(epochs_range, history.history['val_recall'], 'r-', label='Val Recall')\n",
        "        axes[1, 0].axvline(x=best_epoch_idx + 1, color='g', linestyle='--', label=f'Best Epoch ({best_epoch_idx + 1})')\n",
        "        axes[1, 0].scatter([best_epoch_idx + 1], [best_val_recall], color='g', s=100, zorder=5)\n",
        "        axes[1, 0].set_title(f'Recall (Best: {best_val_recall:.4f})')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Precision\n",
        "        axes[1, 1].plot(epochs_range, history.history['precision'], 'b-', label='Train Precision')\n",
        "        axes[1, 1].plot(epochs_range, history.history['val_precision'], 'r-', label='Val Precision')\n",
        "        axes[1, 1].axvline(x=best_epoch_idx + 1, color='g', linestyle='--', label=f'Best Epoch ({best_epoch_idx + 1})')\n",
        "        axes[1, 1].scatter([best_epoch_idx + 1], [best_val_precision], color='g', s=100, zorder=5)\n",
        "        axes[1, 1].set_title(f'Precision (Best: {best_val_precision:.4f})')\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.suptitle(f\"{config['name']} - Training Curves (Best Epoch: {best_epoch_idx + 1})\", fontsize=14)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save and log to W&B\n",
        "        curves_path = f\"curves_{config['name']}.png\"\n",
        "        plt.savefig(curves_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"üìà Training curves saved: {curves_path}\")\n",
        "\n",
        "        wandb.log({'training_curves': wandb.Image(curves_path)})\n",
        "\n",
        "        # Log confusion matrix as image\n",
        "        wandb.log({'confusion_matrix': wandb.Image(save_path)})\n",
        "\n",
        "        # Log interactive confusion matrix\n",
        "        wandb.log({\n",
        "            'conf_mat_interactive': wandb.plot.confusion_matrix(\n",
        "                y_true=y_true,\n",
        "                preds=y_pred,\n",
        "                class_names=['Benign', 'Malignant']\n",
        "            )\n",
        "        })\n",
        "\n",
        "        wandb.finish()\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # 9. PREPARE RESULTS DICTIONARY\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    results = {\n",
        "        'name': config['name'],\n",
        "        'architecture': arch,\n",
        "        'best_epoch': best_epoch_idx + 1,\n",
        "        'best_val_f2': best_val_f2,\n",
        "        'best_val_recall': best_val_recall,\n",
        "        'best_val_precision': best_val_precision,\n",
        "        'best_val_auc': best_val_auc,\n",
        "        'f2_from_cm': f2_cm,\n",
        "        'tn': tn,\n",
        "        'fp': fp,\n",
        "        'fn': fn,\n",
        "        'tp': tp,\n",
        "        'config': config\n",
        "    }\n",
        "\n",
        "    print(f\"\\n‚úÖ Results Logged. Best Epoch: {best_epoch_idx + 1} (by val_f2_score)\")\n",
        "    print(f\"   Val F2:        {best_val_f2:.4f}  ‚Üê Early Stopping Metric\")\n",
        "    print(f\"   Val Recall:    {best_val_recall:.4f}\")\n",
        "    print(f\"   Val Precision: {best_val_precision:.4f}\")\n",
        "    print(f\"   Val AUC:       {best_val_auc:.4f}\")\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # 10. CLEANUP\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    return results\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# HELPER: CONFUSION MATRIX PLOT\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def plot_confusion_matrix(cm, title, save_path, recall=None, precision=None, f2=None):\n",
        "    \"\"\"\n",
        "    Plot and save a confusion matrix with metrics.\n",
        "\n",
        "    Args:\n",
        "        cm: 2x2 confusion matrix\n",
        "        title: Plot title\n",
        "        save_path: Path to save the figure\n",
        "        recall, precision, f2: Optional metrics to display\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Benign', 'Malignant'],\n",
        "                yticklabels=['Benign', 'Malignant'],\n",
        "                annot_kws={'size': 16})\n",
        "\n",
        "    plt.xlabel('Predicted', fontsize=12)\n",
        "    plt.ylabel('Actual', fontsize=12)\n",
        "\n",
        "    title_text = f'{title}'\n",
        "    if f2 is not None:\n",
        "        title_text += f'\\nF2={f2:.3f}'\n",
        "    if recall is not None:\n",
        "        title_text += f' | Recall={recall:.3f}'\n",
        "    if precision is not None:\n",
        "        title_text += f' | Precision={precision:.3f}'\n",
        "\n",
        "    plt.title(title_text, fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ BLOCK 5: TRAINING LOOP LOADED\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\"\"\n",
        "   train_experiment(config, data, datagen)\n",
        "\n",
        "   Supported architectures:\n",
        "   ‚îú‚îÄ‚îÄ 'custom_cnn'  ‚Üí build_custom_cnn()\n",
        "   ‚îú‚îÄ‚îÄ 'hybrid_cnn'  ‚Üí build_hybrid_cnn()\n",
        "   ‚îî‚îÄ‚îÄ 'transfer'    ‚Üí build_transfer_model()\n",
        "\n",
        "   Features:\n",
        "   ‚îú‚îÄ‚îÄ F2-Score Early Stopping\n",
        "   ‚îú‚îÄ‚îÄ W&B Logging (if enabled)\n",
        "   ‚îú‚îÄ‚îÄ Confusion Matrix Generation\n",
        "   ‚îî‚îÄ‚îÄ Automatic Cleanup\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "Zcbm6KIqlspt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 6: Phase 1 ‚Äî Architecture Screening\n",
        "\n",
        "## Goal\n",
        "Find the best **architecture + fine-tuning strategy** combination for malignant lesion detection.\n",
        "\n",
        "## Experimental Design (14 runs)\n",
        "\n",
        "| Architecture   | Variants                                                 | Runs | Parameters |\n",
        "|----------------|----------------------------------------------------------|------|------------|\n",
        "| Custom CNN     | shallow (d=3, f=32), deep (d=4, f=64), wide (d=3, f=128) | 3    | ~50K-200K  |\n",
        "| Hybrid CNN     | with CBAM, without CBAM                                  | 2    | ~80K-100K  |\n",
        "| MobileNetV2    | frozen, partial (50 layers), full                        | 3    | 3.4M       |\n",
        "| EfficientNetB0 | frozen, partial (100 layers), full                       | 3    | 5.3M       |\n",
        "| DenseNet121    | frozen, partial (50 layers), full                        | 3    | 8M         |\n",
        "| **Total**      |                                                          | **14** |          |\n",
        "\n",
        "## Fixed Parameters\n",
        "\n",
        "All experiments use **W3** class weighting: `{0: 1.0, 1: 3.0}` ‚Äî penalizes missed malignant cases 3√ó more than false alarms.\n",
        "\n",
        "## Selection Criterion\n",
        "\n",
        "Best model selected by **validation F2 score** (weights recall 2√ó over precision)."
      ],
      "metadata": {
        "id": "iHfJihSB82dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# BLOCK 6: Phase 1 ‚Äî Architecture Screening\n",
        "# ============================================================================\n",
        "\n",
        "WEIGHT_TAG = \"W3\"\n",
        "data['class_weights'] = {0: 1.0, 1: 3.0}\n",
        "\n",
        "print(f\"‚úÖ Class weights set: {data['class_weights']}\")\n",
        "\n",
        "phase1_results = []"
      ],
      "metadata": {
        "id": "_sjsR7wC_Srd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 6a: Custom CNN (3 runs)\n",
        "\n",
        "Tests three CNN configurations varying depth and width to find optimal complexity for 28√ó28 images.\n",
        "\n",
        "| Name | Depth | Filters | Hypothesis |\n",
        "|------|-------|---------|------------|\n",
        "| P1_CNN_shallow | 3 | 32 | Simple may be sufficient |\n",
        "| P1_CNN_deep | 4 | 64 | More depth = more features |\n",
        "| P1_CNN_wide | 3 | 128 | More filters = more capacity |\n",
        "\n",
        "**Why Custom CNN?**\n",
        "- Native 28√ó28 input (no upsampling artifacts like transfer learning)\n",
        "- Appropriate complexity for ~7K training samples (avoids overfitting)\n",
        "- Fast training and inference"
      ],
      "metadata": {
        "id": "Wi4fbePEq0_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 6a: CUSTOM CNN (3 runs)\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"‚úÖ Running Custom CNN experiments (3 runs)\")\n",
        "\n",
        "configs_cnn = [\n",
        "    {\n",
        "        # Shallow: Tests if simple architecture suffices for low-resolution images\n",
        "        'name': f'P1_CNN_shallow_{WEIGHT_TAG}',\n",
        "        'architecture': 'custom_cnn',\n",
        "        'filters_base': 32,\n",
        "        'depth': 3,\n",
        "        'dropout': 0.4,\n",
        "        'dense_units': 128,\n",
        "        'learning_rate': 0.0005,\n",
        "        'batch_size': 128,\n",
        "        'epochs': 50,\n",
        "        'patience': 10,\n",
        "    },\n",
        "    {\n",
        "        # Deep: Tests if additional layers capture more discriminative features\n",
        "        'name': f'P1_CNN_deep_{WEIGHT_TAG}',\n",
        "        'architecture': 'custom_cnn',\n",
        "        'filters_base': 64,\n",
        "        'depth': 4,\n",
        "        'dropout': 0.4,\n",
        "        'dense_units': 256,\n",
        "        'learning_rate': 0.0005,\n",
        "        'batch_size': 128,\n",
        "        'epochs': 50,\n",
        "        'patience': 10,\n",
        "    },\n",
        "    {\n",
        "        # Wide: Tests if more filters per layer improve feature extraction\n",
        "        'name': f'P1_CNN_wide_{WEIGHT_TAG}',\n",
        "        'architecture': 'custom_cnn',\n",
        "        'filters_base': 128,\n",
        "        'depth': 3,\n",
        "        'dropout': 0.5,  # Higher dropout to compensate for increased capacity\n",
        "        'dense_units': 256,\n",
        "        'learning_rate': 0.0005,\n",
        "        'batch_size': 128,\n",
        "        'epochs': 50,\n",
        "        'patience': 10,\n",
        "    },\n",
        "]\n",
        "\n",
        "for i, config in enumerate(configs_cnn, 1):\n",
        "    result = train_experiment(config, data, datagen)\n",
        "    phase1_results.append(result)\n",
        "    save_to_drive(config['name'], DIR_PHASE_1)"
      ],
      "metadata": {
        "id": "FK0sa1afmCIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 6b: Hybrid CNN with CBAM (2 runs)\n",
        "\n",
        "Tests whether attention mechanisms improve feature extraction, inspired by MedNet (state-of-the-art on DermaMNIST).\n",
        "\n",
        "| Name | CBAM | Purpose |\n",
        "|------|------|---------|\n",
        "| P1_Hybrid_CBAM | Yes | Full attention mechanism |\n",
        "| P1_Hybrid_NoCBAM | No | Ablation study |\n",
        "\n",
        "**CBAM Architecture:**\n",
        "```\n",
        "Channel Attention ‚Üí \"What\" features are important?\n",
        "       ‚Üì\n",
        "Spatial Attention ‚Üí \"Where\" to focus?\n",
        "```\n",
        "\n",
        "**Why CBAM?**\n",
        "- Proven effective for medical imaging (MedNet: 0.840 acc on DermaMNIST)\n",
        "- Lightweight addition (~1% extra parameters)\n",
        "- Ablation study validates attention contribution"
      ],
      "metadata": {
        "id": "VbMoUQTqrJfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 6b: HYBRID CNN WITH CBAM (2 runs)\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"‚úÖ Running Hybrid CNN experiments (2 runs)\")\n",
        "\n",
        "configs_hybrid = [\n",
        "    {\n",
        "        # With CBAM: Tests if attention mechanism improves feature focus\n",
        "        'name': f'P1_Hybrid_CBAM_{WEIGHT_TAG}',\n",
        "        'architecture': 'hybrid_cnn',\n",
        "        'filters_base': 64,\n",
        "        'depth': 3,\n",
        "        'dropout': 0.5,\n",
        "        'dense_units': 256,\n",
        "        'use_cbam': True,\n",
        "        'learning_rate': 0.0005,\n",
        "        'batch_size': 128,\n",
        "        'epochs': 50,\n",
        "        'patience': 10,\n",
        "    },\n",
        "    {\n",
        "        # Without CBAM: Ablation study to isolate attention contribution\n",
        "        'name': f'P1_Hybrid_NoCBAM_{WEIGHT_TAG}',\n",
        "        'architecture': 'hybrid_cnn',\n",
        "        'filters_base': 64,\n",
        "        'depth': 3,\n",
        "        'dropout': 0.5,\n",
        "        'dense_units': 256,\n",
        "        'use_cbam': False,\n",
        "        'learning_rate': 0.0005,\n",
        "        'batch_size': 128,\n",
        "        'epochs': 50,\n",
        "        'patience': 10,\n",
        "    },\n",
        "]\n",
        "\n",
        "for i, config in enumerate(configs_hybrid, 1):\n",
        "    result = train_experiment(config, data, datagen)\n",
        "    phase1_results.append(result)\n",
        "    save_to_drive(config['name'], DIR_PHASE_1)"
      ],
      "metadata": {
        "id": "O28CDEKjmECS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 6c: MobileNetV2 Transfer Learning (3 runs)\n",
        "\n",
        "Tests transfer learning with a lightweight architecture designed for mobile/edge deployment.\n",
        "\n",
        "| Name | Unfrozen Layers | Purpose |\n",
        "|------|-----------------|---------|\n",
        "| P1_MobileNetV2_frozen | 0 | Feature extraction only |\n",
        "| P1_MobileNetV2_partial | 50 | Balance: preserve ImageNet features + adapt |\n",
        "| P1_MobileNetV2_full | All | Full fine-tuning (risk: catastrophic forgetting) |\n",
        "\n",
        "**Why MobileNetV2?**\n",
        "```\n",
        "‚îú‚îÄ‚îÄ Parameters: 3.4M (smallest transfer model)\n",
        "‚îú‚îÄ‚îÄ Inverted residuals + depthwise separable convolutions\n",
        "‚îî‚îÄ‚îÄ Min input 32√ó32 (works well at 56√ó56)\n",
        "```"
      ],
      "metadata": {
        "id": "aLYJiMeurnv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 6c: MOBILENETV2 TRANSFER LEARNING (3 runs)\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"‚úÖ Running MobileNetV2 experiments (3 runs)\")\n",
        "\n",
        "configs_mobilenet = [\n",
        "    {\n",
        "        # Frozen: Uses ImageNet features as fixed feature extractor\n",
        "        'name': f'P1_MobileNetV2_frozen_{WEIGHT_TAG}',\n",
        "        'architecture': 'transfer',\n",
        "        'base_name': 'mobilenetv2',\n",
        "        'dropout': 0.5,\n",
        "        'dense_units': 256,\n",
        "        'unfreeze_layers': 0,\n",
        "        'learning_rate': 0.0005,\n",
        "        'batch_size': 64,\n",
        "        'epochs': 30,\n",
        "        'patience': 10,\n",
        "    },\n",
        "    {\n",
        "        # Partial: Adapts top layers while preserving low-level features\n",
        "        'name': f'P1_MobileNetV2_partial_{WEIGHT_TAG}',\n",
        "        'architecture': 'transfer',\n",
        "        'base_name': 'mobilenetv2',\n",
        "        'dropout': 0.5,\n",
        "        'dense_units': 256,\n",
        "        'unfreeze_layers': 50,\n",
        "        'learning_rate': 0.0001,\n",
        "        'batch_size': 64,\n",
        "        'epochs': 30,\n",
        "        'patience': 10,\n",
        "    },\n",
        "    {\n",
        "        # Full: Complete fine-tuning, lower LR to prevent catastrophic forgetting\n",
        "        'name': f'P1_MobileNetV2_full_{WEIGHT_TAG}',\n",
        "        'architecture': 'transfer',\n",
        "        'base_name': 'mobilenetv2',\n",
        "        'dropout': 0.5,\n",
        "        'dense_units': 256,\n",
        "        'unfreeze_layers': None,\n",
        "        'learning_rate': 0.0001,\n",
        "        'batch_size': 64,\n",
        "        'epochs': 30,\n",
        "        'patience': 10,\n",
        "    },\n",
        "]\n",
        "\n",
        "for i, config in enumerate(configs_mobilenet, 1):\n",
        "    result = train_experiment(config, data, datagen)\n",
        "    phase1_results.append(result)\n",
        "    save_to_drive(config['name'], DIR_PHASE_1)"
      ],
      "metadata": {
        "id": "HLbCYN07sVKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 6d: EfficientNetB0 Transfer Learning (3 runs)\n",
        "\n",
        "Tests compound-scaled architecture using Neural Architecture Search (NAS).\n",
        "\n",
        "| Name | Unfrozen Layers | Purpose |\n",
        "|------|-----------------|---------|\n",
        "| P1_EfficientNet_frozen | 0 | Feature extraction only |\n",
        "| P1_EfficientNet_partial | 100 | Partial adaptation |\n",
        "| P1_EfficientNet_full | All | Full fine-tuning |\n",
        "\n",
        "**Why EfficientNetB0?**\n",
        "```\n",
        "‚îú‚îÄ‚îÄ Parameters: 5.3M (efficient scaling)\n",
        "‚îú‚îÄ‚îÄ Compound scaling (depth, width, resolution)\n",
        "‚îî‚îÄ‚îÄ State-of-the-art efficiency\n",
        "```\n",
        "\n",
        "‚ö†Ô∏è **Caveat:** Designed for 224√ó224 input ‚Äî may underperform at 56√ó56."
      ],
      "metadata": {
        "id": "5padYyAXsE6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 6d: EFFICIENTNETB0 TRANSFER LEARNING (3 runs)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üì¶ 6d: EFFICIENTNETB0 TRANSFER LEARNING (3 runs)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\"\"\n",
        "   EfficientNetB0 Architecture:\n",
        "   ‚îú‚îÄ‚îÄ Parameters: 5.3M (efficient scaling)\n",
        "   ‚îú‚îÄ‚îÄ Uses compound scaling (depth, width, resolution)\n",
        "   ‚îî‚îÄ‚îÄ State-of-the-art efficiency\n",
        "\n",
        "   Fine-tuning strategies:\n",
        "   ‚îú‚îÄ‚îÄ frozen:  All layers frozen (feature extraction only)\n",
        "   ‚îú‚îÄ‚îÄ partial: Last 100 layers trainable\n",
        "   ‚îî‚îÄ‚îÄ full:    All layers trainable\n",
        "\n",
        "   Note: Lower LR (0.0001) and smaller batch (64) for pre-trained weights\n",
        "\"\"\")\n",
        "\n",
        "configs_efficientnet = [\n",
        "    {\n",
        "        'name': f'P1_EfficientNet_frozen_{WEIGHT_TAG}',\n",
        "        'architecture': 'transfer',\n",
        "        'base_name': 'efficientnet',\n",
        "        'description': 'EfficientNetB0 Frozen (feature extraction)',\n",
        "        'dropout': 0.5,\n",
        "        'dense_units': 256,\n",
        "        'unfreeze_layers': 0,  # All frozen\n",
        "        'learning_rate': 0.0005,  # Higher LR ok for frozen base\n",
        "        'batch_size': 64,\n",
        "        'epochs': 30,\n",
        "        'patience': 10,\n",
        "    },\n",
        "    {\n",
        "        'name': f'P1_EfficientNet_partial_{WEIGHT_TAG}',\n",
        "        'architecture': 'transfer',\n",
        "        'base_name': 'efficientnet',\n",
        "        'description': 'EfficientNetB0 Partial (100 layers unfrozen)',\n",
        "        'dropout': 0.5,\n",
        "        'dense_units': 256,\n",
        "        'unfreeze_layers': 100,  # Last 100 layers trainable\n",
        "        'learning_rate': 0.0001,  # Lower LR for fine-tuning\n",
        "        'batch_size': 64,\n",
        "        'epochs': 30,\n",
        "        'patience': 10,\n",
        "    },\n",
        "    {\n",
        "        'name': f'P1_EfficientNet_full_{WEIGHT_TAG}',\n",
        "        'architecture': 'transfer',\n",
        "        'base_name': 'efficientnet',\n",
        "        'description': 'EfficientNetB0 Full (all layers unfrozen)',\n",
        "        'dropout': 0.5,\n",
        "        'dense_units': 256,\n",
        "        'unfreeze_layers': None,  # All trainable\n",
        "        'learning_rate': 0.0001,  # Lower LR for fine-tuning\n",
        "        'batch_size': 64,\n",
        "        'epochs': 30,\n",
        "        'patience': 10,\n",
        "    },\n",
        "]\n",
        "\n",
        "for i, config in enumerate(configs_efficientnet, 1):\n",
        "    print(f\"\\n‚ñ∂Ô∏è EfficientNetB0 [{i}/3] {config['description']}\")\n",
        "    result = train_experiment(config, data, datagen)\n",
        "    phase1_results.append(result)\n",
        "    save_to_drive(config['name'], DIR_PHASE_1)"
      ],
      "metadata": {
        "id": "mB8atNOGmG8l",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 6e: DenseNet121 Transfer Learning (3 runs)\n",
        "\n",
        "Tests dense connectivity architecture with feature reuse across layers.\n",
        "\n",
        "| Name | Unfrozen Layers | Purpose |\n",
        "|------|-----------------|---------|\n",
        "| P1_DenseNet_frozen | 0 | Feature extraction only |\n",
        "| P1_DenseNet_partial | 50 | Partial adaptation |\n",
        "| P1_DenseNet_full | All | Full fine-tuning (stable due to dense connections) |\n",
        "\n",
        "**Why DenseNet121?**\n",
        "```\n",
        "‚îú‚îÄ‚îÄ Parameters: 8M (largest, but efficient feature reuse)\n",
        "‚îú‚îÄ‚îÄ Dense connections prevent information loss at low resolution\n",
        "‚îú‚îÄ‚îÄ Better gradient flow enables stable full fine-tuning\n",
        "‚îî‚îÄ‚îÄ Min input 32√ó32 (works well at 56√ó56)\n",
        "```"
      ],
      "metadata": {
        "id": "2TMhcsqMfvho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 6e: DENSENET121 TRANSFER LEARNING (3 runs)\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"‚úÖ Running DenseNet121 experiments (3 runs)\")\n",
        "\n",
        "configs_densenet = [\n",
        "    {\n",
        "        # Frozen: Tests dense features as fixed extractor\n",
        "        'name': f'P1_DenseNet_frozen_{WEIGHT_TAG}',\n",
        "        'architecture': 'transfer',\n",
        "        'base_name': 'densenet',\n",
        "        'dropout': 0.5,\n",
        "        'dense_units': 256,\n",
        "        'unfreeze_layers': 0,\n",
        "        'learning_rate': 0.0005,\n",
        "        'batch_size': 64,\n",
        "        'epochs': 30,\n",
        "        'patience': 10,\n",
        "    },\n",
        "    {\n",
        "        # Partial: Adapts top dense blocks to medical domain\n",
        "        'name': f'P1_DenseNet_partial_{WEIGHT_TAG}',\n",
        "        'architecture': 'transfer',\n",
        "        'base_name': 'densenet',\n",
        "        'dropout': 0.5,\n",
        "        'dense_units': 256,\n",
        "        'unfreeze_layers': 50,\n",
        "        'learning_rate': 0.0001,\n",
        "        'batch_size': 64,\n",
        "        'epochs': 30,\n",
        "        'patience': 10,\n",
        "    },\n",
        "    {\n",
        "        # Full: Dense connections enable stable full fine-tuning\n",
        "        'name': f'P1_DenseNet_full_{WEIGHT_TAG}',\n",
        "        'architecture': 'transfer',\n",
        "        'base_name': 'densenet',\n",
        "        'dropout': 0.5,\n",
        "        'dense_units': 256,\n",
        "        'unfreeze_layers': None,\n",
        "        'learning_rate': 0.0001,\n",
        "        'batch_size': 64,\n",
        "        'epochs': 30,\n",
        "        'patience': 10,\n",
        "    },\n",
        "]\n",
        "\n",
        "for i, config in enumerate(configs_densenet, 1):\n",
        "    result = train_experiment(config, data, datagen)\n",
        "    phase1_results.append(result)\n",
        "    save_to_drive(config['name'], DIR_PHASE_1)"
      ],
      "metadata": {
        "id": "zftbwvp8fvDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PHASE 1 SUMMARY\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä PHASE 1 COMPLETE - SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "summary_df = pd.DataFrame(phase1_results)\n",
        "summary_df = summary_df.sort_values('best_val_f2', ascending=False)\n",
        "\n",
        "print(\"\\nüèÜ RANKING BY F2 SCORE:\\n\")\n",
        "print(summary_df[['name', 'best_val_f2', 'best_val_recall', 'best_val_precision', 'best_val_auc']].to_string(index=False))\n",
        "\n",
        "# Best model\n",
        "best_model = summary_df.iloc[0]\n",
        "print(f\"\\nü•á BEST MODEL: {best_model['name']}\")\n",
        "print(f\"   F2 Score:  {best_model['best_val_f2']:.4f}\")\n",
        "print(f\"   Recall:    {best_model['best_val_recall']:.4f}\")\n",
        "print(f\"   Precision: {best_model['best_val_precision']:.4f}\")\n",
        "print(f\"   AUC:       {best_model['best_val_auc']:.4f}\")\n",
        "\n",
        "summary_path = os.path.join(DIR_PHASE_1, 'phase1_summary.csv')\n",
        "summary_df.to_csv(summary_path, index=False)\n",
        "print(f\"\\nüíæ Summary saved: {summary_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚û°Ô∏è NEXT: Phase 2 - Hyperparameter Tuning on Best Architecture\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "p19grKPPmQ9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 7: Phase 2 - Hyperparameter Tuning\n",
        "\n",
        "## Goal\n",
        "Fine-tune the Phase 1 winner to maximize validation F2/Recall.\n",
        "\n",
        "## Winner Selection\n",
        "Based on Phase 1 results:\n",
        "- **If highest F2:** DenseNet_partial (F2=0.726)\n",
        "- **If highest Recall:** CNN_shallow (Recall=97.45%, FN=5)\n",
        "\n",
        "For clinical deployment, **CNN_shallow** is preferred due to:\n",
        "1. Lowest false negatives (5 vs 21-29)\n",
        "2. Highest recall (97.45%)\n",
        "3. Appropriate model complexity (~50K params)\n",
        "\n",
        "## Tuning Grid (for CNN)\n",
        "| Parameter | Values |\n",
        "|-----------|--------|\n",
        "| filters_base | 32, 48, 64 |\n",
        "| depth | 3, 4 |\n",
        "| dropout | 0.3, 0.4, 0.5 |\n",
        "| learning_rate | 0.001, 0.0005 |\n",
        "| dense_units | 64, 128, 256 |"
      ],
      "metadata": {
        "id": "bWqViAIT9nz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# GOAL: Fine-tune the winning architecture from Phase 1\n",
        "# Strategy:\n",
        "# - If Transfer Learning won: fine-grained freeze depth tuning\n",
        "# - If CNN won: depth, width, regularization tuning\n",
        "# - Plus: dropout and learning rate experiments\n",
        "# Total: 6 Runs\n",
        "\n",
        "WINNER_ARCH, PHASE1_WEIGHT = get_phase1_winner()\n",
        "\n",
        "if WINNER_ARCH is None:\n",
        "    raise RuntimeError(\"‚ùå Phase 1 must be completed first!\")\n",
        "\n",
        "results = load_results()\n",
        "phase1_winner = results['phase1']['best_model']\n",
        "\n",
        "winner_config = phase1_winner.get('config', {})\n",
        "winner_unfreeze = winner_config.get('unfreeze_layers', None)\n",
        "\n",
        "# Determine freeze strategy from Phase 1\n",
        "if winner_unfreeze == 0:\n",
        "    winner_freeze_type = 'frozen'\n",
        "elif winner_unfreeze is None:\n",
        "    winner_freeze_type = 'full'\n",
        "else:\n",
        "    winner_freeze_type = 'partial'\n",
        "\n",
        "WEIGHT_TAG = \"W3\"\n",
        "data['class_weights'] = {0: 1.0, 1: 3.0}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üîß PHASE 2: HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"   Phase 1 Winner:    {phase1_winner['name']}\")\n",
        "print(f\"   Architecture:      {WINNER_ARCH}\")\n",
        "print(f\"   Freeze Strategy:   {winner_freeze_type}\")\n",
        "print(f\"   Unfreeze Layers:   {winner_unfreeze}\")\n",
        "print(f\"   Phase 1 Recall:    {phase1_winner['val_recall']:.4f}\")\n",
        "print(f\"   Weight Strategy:   W3 (fixed)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "phase2_results = []\n",
        "\n",
        "# ============================================================================\n",
        "# DEFINE TUNING CONFIGURATIONS\n",
        "\n",
        "if WINNER_ARCH == 'custom_cnn':\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # CNN TUNING: Architecture and regularization\n",
        "\n",
        "    print(\"\\nüì¶ CNN Hyperparameter Tuning\")\n",
        "    print(\"   Focus: depth, filters, dropout, learning rate\")\n",
        "\n",
        "    tuning_configs = [\n",
        "        {\n",
        "            'name': f'P2_CNN_Deeper_{WEIGHT_TAG}',\n",
        "            'architecture': 'custom_cnn',\n",
        "            'description': 'Deeper (5 blocks)',\n",
        "            'filters_base': 64, 'depth': 5, 'dropout': 0.4, 'dense_units': 256,\n",
        "            'learning_rate': 0.001, 'batch_size': 64, 'epochs': 50, 'patience': 15,\n",
        "            'unfreeze_layers': None\n",
        "        },\n",
        "        {\n",
        "            'name': f'P2_CNN_Wider_{WEIGHT_TAG}',\n",
        "            'architecture': 'custom_cnn',\n",
        "            'description': 'Wider (128 filters)',\n",
        "            'filters_base': 128, 'depth': 4, 'dropout': 0.4, 'dense_units': 256,\n",
        "            'learning_rate': 0.0005, 'batch_size': 32, 'epochs': 50, 'patience': 15,\n",
        "            'unfreeze_layers': None\n",
        "        },\n",
        "        {\n",
        "            'name': f'P2_CNN_HighDropout_{WEIGHT_TAG}',\n",
        "            'architecture': 'custom_cnn',\n",
        "            'description': 'High dropout (0.6)',\n",
        "            'filters_base': 64, 'depth': 4, 'dropout': 0.6, 'dense_units': 256,\n",
        "            'learning_rate': 0.001, 'batch_size': 64, 'epochs': 50, 'patience': 15,\n",
        "            'unfreeze_layers': None\n",
        "        },\n",
        "        {\n",
        "            'name': f'P2_CNN_LowLR_{WEIGHT_TAG}',\n",
        "            'architecture': 'custom_cnn',\n",
        "            'description': 'Low learning rate (0.0005)',\n",
        "            'filters_base': 64, 'depth': 4, 'dropout': 0.4, 'dense_units': 256,\n",
        "            'learning_rate': 0.0005, 'batch_size': 64, 'epochs': 50, 'patience': 15,\n",
        "            'unfreeze_layers': None\n",
        "        },\n",
        "        {\n",
        "            'name': f'P2_CNN_SmallBatch_{WEIGHT_TAG}',\n",
        "            'architecture': 'custom_cnn',\n",
        "            'description': 'Small batch (32)',\n",
        "            'filters_base': 64, 'depth': 4, 'dropout': 0.4, 'dense_units': 256,\n",
        "            'learning_rate': 0.001, 'batch_size': 32, 'epochs': 50, 'patience': 15,\n",
        "            'unfreeze_layers': None\n",
        "        },\n",
        "        {\n",
        "            'name': f'P2_CNN_BigDense_{WEIGHT_TAG}',\n",
        "            'architecture': 'custom_cnn',\n",
        "            'description': 'Larger dense layer (512)',\n",
        "            'filters_base': 64, 'depth': 4, 'dropout': 0.5, 'dense_units': 512,\n",
        "            'learning_rate': 0.001, 'batch_size': 64, 'epochs': 50, 'patience': 15,\n",
        "            'unfreeze_layers': None\n",
        "        },\n",
        "    ]\n",
        "\n",
        "else:\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # TRANSFER LEARNING TUNING: Fine-grained freeze depth\n",
        "\n",
        "    if WINNER_ARCH == 'resnet50':\n",
        "        total_layers = 175\n",
        "        # Test around Phase 1 winner's value\n",
        "        if winner_freeze_type == 'frozen':\n",
        "            unfreeze_options = [20, 40, 60]  # Start unfreezing\n",
        "        elif winner_freeze_type == 'partial':\n",
        "            base = winner_unfreeze if winner_unfreeze else 80\n",
        "            unfreeze_options = [base-20, base, base+20, base+40]\n",
        "        else:  # full\n",
        "            unfreeze_options = [100, 120, 140, None]  # Around full\n",
        "\n",
        "    elif WINNER_ARCH == 'vgg16':\n",
        "        total_layers = 19\n",
        "        if winner_freeze_type == 'frozen':\n",
        "            unfreeze_options = [4, 8, 12]\n",
        "        elif winner_freeze_type == 'partial':\n",
        "            base = winner_unfreeze if winner_unfreeze else 8\n",
        "            unfreeze_options = [max(4, base-4), base, min(19, base+4)]\n",
        "        else:  # full\n",
        "            unfreeze_options = [12, 15, None]\n",
        "\n",
        "    else:  # efficientnet\n",
        "        total_layers = 237\n",
        "        if winner_freeze_type == 'frozen':\n",
        "            unfreeze_options = [40, 80, 120]\n",
        "        elif winner_freeze_type == 'partial':\n",
        "            base = winner_unfreeze if winner_unfreeze else 100\n",
        "            unfreeze_options = [base-30, base, base+30, base+60]\n",
        "        else:  # full\n",
        "            unfreeze_options = [150, 180, 200, None]\n",
        "\n",
        "    # Remove duplicates and None handling\n",
        "    unfreeze_options = list(dict.fromkeys(unfreeze_options))\n",
        "\n",
        "    print(f\"\\nüì¶ {WINNER_ARCH.upper()} Hyperparameter Tuning\")\n",
        "    print(f\"   Total layers: {total_layers}\")\n",
        "    print(f\"   Phase 1 unfreeze: {winner_unfreeze}\")\n",
        "    print(f\"   Testing unfreeze: {unfreeze_options}\")\n",
        "\n",
        "    tuning_configs = []\n",
        "\n",
        "    # Freeze depth experiments\n",
        "    for unfreeze in unfreeze_options:\n",
        "        if unfreeze is None:\n",
        "            name_suffix = \"Full\"\n",
        "            desc = f\"Full fine-tuning (all {total_layers} layers)\"\n",
        "            lr = 0.00005\n",
        "        else:\n",
        "            name_suffix = f\"Unfreeze{unfreeze}\"\n",
        "            desc = f\"Unfreeze {unfreeze}/{total_layers} layers ({100*unfreeze/total_layers:.0f}%)\"\n",
        "            lr = 0.0001\n",
        "\n",
        "        tuning_configs.append({\n",
        "            'name': f'P2_{WINNER_ARCH}_{name_suffix}_{WEIGHT_TAG}',\n",
        "            'architecture': WINNER_ARCH,\n",
        "            'description': desc,\n",
        "            'dropout': 0.5,\n",
        "            'dense_units': 256,\n",
        "            'unfreeze_layers': unfreeze,\n",
        "            'learning_rate': lr,\n",
        "            'batch_size': 32,\n",
        "            'epochs': 50,\n",
        "            'patience': 15\n",
        "        })\n",
        "\n",
        "    # Add regularization experiments\n",
        "    best_unfreeze = winner_unfreeze if winner_unfreeze else unfreeze_options[len(unfreeze_options)//2]\n",
        "\n",
        "    tuning_configs.extend([\n",
        "        {\n",
        "            'name': f'P2_{WINNER_ARCH}_HighDropout_{WEIGHT_TAG}',\n",
        "            'architecture': WINNER_ARCH,\n",
        "            'description': f'High dropout (0.7) + {best_unfreeze} layers',\n",
        "            'dropout': 0.7,\n",
        "            'dense_units': 256,\n",
        "            'unfreeze_layers': best_unfreeze,\n",
        "            'learning_rate': 0.0001,\n",
        "            'batch_size': 32,\n",
        "            'epochs': 50,\n",
        "            'patience': 15\n",
        "        },\n",
        "        {\n",
        "            'name': f'P2_{WINNER_ARCH}_VeryLowLR_{WEIGHT_TAG}',\n",
        "            'architecture': WINNER_ARCH,\n",
        "            'description': f'Very low LR (0.00003) + {best_unfreeze} layers',\n",
        "            'dropout': 0.5,\n",
        "            'dense_units': 256,\n",
        "            'unfreeze_layers': best_unfreeze,\n",
        "            'learning_rate': 0.00003,\n",
        "            'batch_size': 32,\n",
        "            'epochs': 50,\n",
        "            'patience': 15\n",
        "        },\n",
        "    ])\n",
        "\n",
        "# ============================================================================\n",
        "# RUN TUNING EXPERIMENTS\n",
        "\n",
        "print(f\"\\nüîß Running {len(tuning_configs)} experiments...\")\n",
        "\n",
        "for i, config in enumerate(tuning_configs, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"‚ñ∂Ô∏è [{i}/{len(tuning_configs)}] {config['description']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    result = train_experiment(config, data, datagen)\n",
        "    phase2_results.append(result)\n",
        "    save_to_drive(config['name'], DIR_PHASE_2)\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARE RESULTS\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä PHASE 2 RESULTS (vs Phase 1 Winner)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if phase2_results:\n",
        "    comparison_data = []\n",
        "\n",
        "    # Add Phase 1 winner as baseline\n",
        "    comparison_data.append({\n",
        "        'Name': phase1_winner['name'],\n",
        "        'Description': '‚≠ê Phase 1 Winner (baseline)',\n",
        "        'Unfreeze': winner_unfreeze,\n",
        "        'Val_Recall': phase1_winner['val_recall'],\n",
        "        'Val_AUC': phase1_winner.get('val_auc', 0),\n",
        "        'Source': 'P1'\n",
        "    })\n",
        "\n",
        "    # Add Phase 2 results\n",
        "    for res in phase2_results:\n",
        "        if 'error' in res:\n",
        "            print(f\"‚ö†Ô∏è Skipping {res['config']['name']} (failed)\")\n",
        "            continue\n",
        "\n",
        "        comparison_data.append({\n",
        "            'Name': res['config']['name'],\n",
        "            'Description': res['config'].get('description', ''),\n",
        "            'Unfreeze': res['config'].get('unfreeze_layers', 'N/A'),\n",
        "            'Val_Recall': res.get('best_val_recall', 0),\n",
        "            'Val_AUC': res.get('val_auc', 0),\n",
        "            'Source': 'P2'\n",
        "        })\n",
        "\n",
        "    df_compare = pd.DataFrame(comparison_data)\n",
        "    df_compare = df_compare.sort_values('Val_Recall', ascending=False)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(df_compare.to_string(index=False))\n",
        "\n",
        "    # Find overall best\n",
        "    best_idx = df_compare['Val_Recall'].idxmax()\n",
        "    best_row = df_compare.loc[best_idx]\n",
        "\n",
        "    # Calculate improvement\n",
        "    p1_recall = phase1_winner['val_recall']\n",
        "    p2_recalls = df_compare[df_compare['Source'] == 'P2']['Val_Recall']\n",
        "    p2_best_recall = p2_recalls.max() if len(p2_recalls) > 0 else 0\n",
        "    improvement = p2_best_recall - p1_recall\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    if improvement > 0:\n",
        "        print(f\"‚úÖ Phase 2 IMPROVED over Phase 1: +{improvement:.4f}\")\n",
        "    elif improvement == 0:\n",
        "        print(f\"‚öñÔ∏è  Phase 2 matched Phase 1 (no improvement)\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Phase 2 did NOT improve: {improvement:.4f}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üèÜ OVERALL BEST MODEL\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"   Model:       {best_row['Name']}\")\n",
        "    print(f\"   Source:      Phase {best_row['Source'][-1]}\")\n",
        "    print(f\"   Unfreeze:    {best_row['Unfreeze']}\")\n",
        "    print(f\"   Val Recall:  {best_row['Val_Recall']:.4f} ‚Üê PRIMARY METRIC\")\n",
        "    print(f\"   Val AUC:     {best_row['Val_AUC']:.4f}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    update_phase2_results(phase2_results, WEIGHT_TAG)\n",
        "\n",
        "    csv_path = os.path.join(BASE_PATH, f'phase2_results_{WEIGHT_TAG}.csv')\n",
        "    df_compare.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüíæ Results saved: {csv_path}\")\n",
        "    print(f\"üíæ Models saved:  {DIR_PHASE_2}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ PHASE 2 COMPLETE\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"   ‚Üí Proceed to Block 8 (Phase 3: Final Evaluation)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No results available. Check for errors above.\")"
      ],
      "metadata": {
        "id": "gpLhMwffvnNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 8: Phase 3 - Final Evaluation\n",
        "\n",
        "## Goal\n",
        "Unbiased evaluation on the held-out **TEST set** (used only once!).\n",
        "\n",
        "## Pipeline\n",
        "\n",
        "| Step | Action | Data Split |\n",
        "|------|--------|------------|\n",
        "| 1 | Load best model from Phase 2 | - |\n",
        "| 2 | Threshold calibration (optional) | Validation |\n",
        "| 3 | Final predictions | **Test** |\n",
        "| 4 | Compute confusion matrix | **Test** |\n",
        "| 5 | Clinical interpretation | - |\n",
        "\n",
        "## Threshold Calibration\n",
        "Default threshold: 0.5\n",
        "Can be adjusted to optimize Recall vs Precision trade-off:\n",
        "- Lower threshold ‚Üí Higher Recall, More FP\n",
        "- Higher threshold ‚Üí Lower Recall, Fewer FP\n",
        "\n",
        "## Final Metrics\n",
        "- Recall (primary): % of malignant cases detected\n",
        "- F2 Score: Weighted harmonic mean\n",
        "- Confusion Matrix: TP, FP, TN, FN\n",
        "- Clinical interpretation: Missed cancers vs unnecessary biopsies"
      ],
      "metadata": {
        "id": "0c3IelWj9wqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# GOAL: Final unbiased evaluation on TEST set\n",
        "# Steps:\n",
        "\n",
        "results = load_results()\n",
        "\n",
        "if not results['phase1']['completed']:\n",
        "    raise RuntimeError(\"‚ùå Phase 1 must be completed first!\")\n",
        "\n",
        "# Phase 1 best\n",
        "p1_best = results['phase1']['best_model']\n",
        "p1_name = p1_best['name']\n",
        "p1_recall = p1_best['val_recall']\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üèÅ PHASE 3: FINAL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüìä PHASE COMPARISON:\")\n",
        "print(\"-\"*50)\n",
        "print(f\"   Phase 1 Best: {p1_name}\")\n",
        "print(f\"   Val Recall:   {p1_recall:.4f}\")\n",
        "\n",
        "# Check Phase 2\n",
        "if results['phase2']['completed']:\n",
        "    p2_best = results['phase2']['best_model']\n",
        "    p2_name = p2_best['name']\n",
        "    p2_recall = p2_best['val_recall']\n",
        "\n",
        "    print(f\"\\n   Phase 2 Best: {p2_name}\")\n",
        "    print(f\"   Val Recall:   {p2_recall:.4f}\")\n",
        "\n",
        "    improvement = p2_recall - p1_recall\n",
        "    print(f\"\\n   Improvement:  {improvement:+.4f}\")\n",
        "\n",
        "    # Auto-select best\n",
        "    if p2_recall >= p1_recall:\n",
        "        FINAL_MODEL_NAME = p2_name\n",
        "        FINAL_MODEL_RECALL = p2_recall\n",
        "        SOURCE_PHASE = 2\n",
        "        SOURCE_DIR = DIR_PHASE_2\n",
        "        print(f\"\\n   ‚úÖ Using Phase 2 model (better or equal)\")\n",
        "    else:\n",
        "        FINAL_MODEL_NAME = p1_name\n",
        "        FINAL_MODEL_RECALL = p1_recall\n",
        "        SOURCE_PHASE = 1\n",
        "        SOURCE_DIR = DIR_PHASE_1\n",
        "        print(f\"\\n   ‚ö†Ô∏è Using Phase 1 model (Phase 2 didn't improve)\")\n",
        "else:\n",
        "    print(f\"\\n   Phase 2: Not completed\")\n",
        "    FINAL_MODEL_NAME = p1_name\n",
        "    FINAL_MODEL_RECALL = p1_recall\n",
        "    SOURCE_PHASE = 1\n",
        "    SOURCE_DIR = DIR_PHASE_1\n",
        "    print(f\"\\n   ‚Üí Using Phase 1 model\")\n",
        "\n",
        "print(\"-\"*50)\n",
        "\n",
        "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "WEIGHT_TAG = FINAL_MODEL_NAME.split('_')[-1]\n",
        "final_model_path = os.path.join(SOURCE_DIR, f'{FINAL_MODEL_NAME}.keras')\n",
        "\n",
        "print(f\"\\nüì• LOADING MODEL:\")\n",
        "print(f\"   Name:   {FINAL_MODEL_NAME}\")\n",
        "print(f\"   Source: Phase {SOURCE_PHASE}\")\n",
        "print(f\"   Path:   {final_model_path}\")\n",
        "\n",
        "if not os.path.exists(final_model_path):\n",
        "    raise FileNotFoundError(f\"‚ùå Model not found: {final_model_path}\")\n",
        "\n",
        "final_model = tf.keras.models.load_model(final_model_path)\n",
        "print(\"   ‚úÖ Model loaded successfully\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéØ THRESHOLD CALIBRATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"   Dataset:    VALIDATION (never test set!)\")\n",
        "print(\"   Goal:       Maximize Recall while keeping Precision ‚â• 40%\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "val_probs = final_model.predict(data['x_val'], verbose=0)\n",
        "val_true = np.argmax(data['y_val'], axis=1)\n",
        "val_scores = val_probs[:, 1]\n",
        "\n",
        "thresholds = np.arange(0.10, 0.90, 0.05)\n",
        "best_threshold = 0.5\n",
        "best_recall = 0.0\n",
        "min_precision = 0.40\n",
        "\n",
        "print(f\"\\n   {'Threshold':<12} {'Recall':<10} {'Precision':<10} {'Status'}\")\n",
        "print(f\"   {'-'*45}\")\n",
        "\n",
        "for t in thresholds:\n",
        "    preds = (val_scores >= t).astype(int)\n",
        "    rec = recall_score(val_true, preds)\n",
        "    prec = precision_score(val_true, preds, zero_division=0)\n",
        "\n",
        "    if prec < min_precision:\n",
        "        status = \"‚ùå Low precision\"\n",
        "    elif rec > best_recall:\n",
        "        status = \"‚úÖ NEW BEST\"\n",
        "        best_recall = rec\n",
        "        best_threshold = t\n",
        "    else:\n",
        "        status = \"\"\n",
        "\n",
        "    print(f\"   {t:<12.2f} {rec:<10.4f} {prec:<10.4f} {status}\")\n",
        "\n",
        "print(f\"\\n   ‚úÖ Optimal Threshold: {best_threshold:.2f}\")\n",
        "print(f\"      Expected Recall:  {best_recall:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üß™ FINAL TEST EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"   ‚ö†Ô∏è  TEST SET - Used only ONCE for final, unbiased evaluation\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "test_probs = final_model.predict(data['x_test'], verbose=0)\n",
        "test_true = np.argmax(data['y_test'], axis=1)\n",
        "test_scores = test_probs[:, 1]\n",
        "test_preds = (test_scores >= best_threshold).astype(int)\n",
        "\n",
        "# Metrics\n",
        "final_recall = recall_score(test_true, test_preds)\n",
        "final_precision = precision_score(test_true, test_preds)\n",
        "final_f1 = f1_score(test_true, test_preds)\n",
        "final_auc = roc_auc_score(test_true, test_scores)\n",
        "cm = confusion_matrix(test_true, test_preds)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"üèÜ FINAL RESULTS: {FINAL_MODEL_NAME}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\"\"\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ  PRIMARY METRIC                                             ‚îÇ\n",
        "   ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ\n",
        "   ‚îÇ  Recall (Sensitivity):  {final_recall:.4f}  ({100*final_recall:.1f}%)               ‚îÇ\n",
        "   ‚îÇ                                                             ‚îÇ\n",
        "   ‚îÇ  SECONDARY METRICS                                          ‚îÇ\n",
        "   ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ\n",
        "   ‚îÇ  Precision:             {final_precision:.4f}  ({100*final_precision:.1f}%)               ‚îÇ\n",
        "   ‚îÇ  F1 Score:              {final_f1:.4f}                           ‚îÇ\n",
        "   ‚îÇ  AUC:                   {final_auc:.4f}                           ‚îÇ\n",
        "   ‚îÇ  Threshold:             {best_threshold:.2f}                             ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\"\"\")\n",
        "\n",
        "print(\"   üéØ CONFUSION MATRIX:\")\n",
        "print(f\"\"\"\n",
        "                        Predicted\n",
        "                     Benign    Malignant\n",
        "                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   Actual Benign   ‚îÇ  {tn:5d}  ‚îÇ  {fp:5d}  ‚îÇ\n",
        "                   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "   Actual Malig.   ‚îÇ  {fn:5d}  ‚îÇ  {tp:5d}  ‚îÇ\n",
        "                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\"\"\")\n",
        "\n",
        "total_malignant = fn + tp\n",
        "total_benign = tn + fp\n",
        "\n",
        "print(\"   üè• CLINICAL INTERPRETATION:\")\n",
        "print(f\"\"\"\n",
        "   Malignant Cases: {total_malignant}\n",
        "   ‚îú‚îÄ‚îÄ ‚úÖ Detected (True Positive):   {tp:4d}  ({100*tp/total_malignant:5.1f}%)\n",
        "   ‚îî‚îÄ‚îÄ ‚ùå Missed (False Negative):    {fn:4d}  ({100*fn/total_malignant:5.1f}%)  ‚Üê DANGEROUS\n",
        "\n",
        "   Benign Cases: {total_benign}\n",
        "   ‚îú‚îÄ‚îÄ ‚úÖ Correct (True Negative):    {tn:4d}  ({100*tn/total_benign:5.1f}%)\n",
        "   ‚îî‚îÄ‚îÄ ‚ö†Ô∏è  False Alarm (False Pos):   {fp:4d}  ({100*fp/total_benign:5.1f}%)  ‚Üí unnecessary biopsy\n",
        "\"\"\")\n",
        "\n",
        "cm_path = os.path.join(DIR_PHASE_3, f'FINAL_confusion_matrix.png')\n",
        "plot_confusion_matrix(cm, f'FINAL: {FINAL_MODEL_NAME}', save_path=cm_path)\n",
        "\n",
        "final_results = {\n",
        "    'model_name': FINAL_MODEL_NAME,\n",
        "    'source_phase': SOURCE_PHASE,\n",
        "    'weight_strategy': WEIGHT_TAG,\n",
        "    'threshold': best_threshold,\n",
        "    'test_recall': final_recall,\n",
        "    'test_precision': final_precision,\n",
        "    'test_f1': final_f1,\n",
        "    'test_auc': final_auc,\n",
        "    'true_neg': int(tn),\n",
        "    'false_pos': int(fp),\n",
        "    'false_neg': int(fn),\n",
        "    'true_pos': int(tp),\n",
        "    'total_malignant': int(total_malignant),\n",
        "    'total_benign': int(total_benign),\n",
        "    'missed_cancers': int(fn),\n",
        "    'detection_rate': float(tp/total_malignant)\n",
        "}\n",
        "\n",
        "# Update tracking\n",
        "results['phase3']['completed'] = True\n",
        "results['phase3']['final_results'] = final_results\n",
        "save_results(results)\n",
        "\n",
        "df_final = pd.DataFrame([final_results])\n",
        "csv_path = os.path.join(DIR_PHASE_3, f'FINAL_RESULTS.csv')\n",
        "df_final.to_csv(csv_path, index=False)\n",
        "\n",
        "# Copy model\n",
        "model_dest = os.path.join(DIR_PHASE_3, f'FINAL_MODEL.keras')\n",
        "shutil.copy(final_model_path, model_dest)\n",
        "\n",
        "print(f\"\\nüíæ FILES SAVED:\")\n",
        "print(f\"   Results:  {csv_path}\")\n",
        "print(f\"   Model:    {model_dest}\")\n",
        "print(f\"   CM Plot:  {cm_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ALL EXPERIMENTS COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "show_experiment_status()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìã FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\"\"\n",
        "   Model:           {FINAL_MODEL_NAME}\n",
        "   Source:          Phase {SOURCE_PHASE}\n",
        "\n",
        "   Test Recall:     {final_recall:.4f} ({100*final_recall:.1f}%)\n",
        "   Test Precision:  {final_precision:.4f} ({100*final_precision:.1f}%)\n",
        "   Test AUC:        {final_auc:.4f}\n",
        "\n",
        "   Missed Cancers:  {fn} out of {total_malignant} ({100*fn/total_malignant:.1f}%)\n",
        "\"\"\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "flLA3Wwivqeb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}